<?xml version='1.0' encoding='UTF-8'?>
<pmc-articleset><article xmlns:mml="http://www.w3.org/1998/Math/MathML" xmlns:xlink="http://www.w3.org/1999/xlink" dtd-version="1.3" xml:lang="en" article-type="research-article">
  <?properties open_access?>
  <processing-meta base-tagset="archiving" mathml-version="3.0" table-model="xhtml" tagset-family="jats">
    <restricted-by>pmc</restricted-by>
  </processing-meta>
  <front>
    <journal-meta>
      <journal-id journal-id-type="nlm-ta">Nat Sci Sleep</journal-id>
      <journal-id journal-id-type="iso-abbrev">Nat Sci Sleep</journal-id>
      <journal-id journal-id-type="publisher-id">nss</journal-id>
      <journal-title-group>
        <journal-title>Nature and Science of Sleep</journal-title>
      </journal-title-group>
      <issn pub-type="epub">1179-1608</issn>
      <publisher>
        <publisher-name>Dove</publisher-name>
      </publisher>
    </journal-meta>
    <article-meta>
      <article-id pub-id-type="pmid">35637772</article-id>
      <article-id pub-id-type="pmc">9148176</article-id>
      <article-id pub-id-type="publisher-id">355702</article-id>
      <article-id pub-id-type="doi">10.2147/NSS.S355702</article-id>
      <article-categories>
        <subj-group subj-group-type="heading">
          <subject>Original Research</subject>
        </subj-group>
      </article-categories>
      <title-group>
        <article-title>Sleep Stage Classification Based on Multi-Centers: Comparison Between Different Ages, Mental Health Conditions and Acquisition Devices</article-title>
        <alt-title alt-title-type="running-authors">Xu et al</alt-title>
        <alt-title alt-title-type="running-title">Xu et al</alt-title>
      </title-group>
      <contrib-group>
        <contrib contrib-type="author" equal-contrib="yes">
          <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0002-8618-2209</contrib-id>
          <name>
            <surname>Xu</surname>
            <given-names>Ziliang</given-names>
          </name>
          <xref rid="aff0001" ref-type="aff">
<sup>1</sup>
</xref>
          <xref rid="ft0001" ref-type="author-notes">*</xref>
        </contrib>
        <contrib contrib-type="author" equal-contrib="yes">
          <contrib-id contrib-id-type="orcid">http://orcid.org/0000-0001-7685-1539</contrib-id>
          <name>
            <surname>Zhu</surname>
            <given-names>Yuanqiang</given-names>
          </name>
          <xref rid="aff0001" ref-type="aff">
<sup>1</sup>
</xref>
          <xref rid="ft0001" ref-type="author-notes">*</xref>
        </contrib>
        <contrib contrib-type="author" equal-contrib="yes">
          <name>
            <surname>Zhao</surname>
            <given-names>Hongliang</given-names>
          </name>
          <xref rid="aff0001" ref-type="aff">
<sup>1</sup>
</xref>
          <xref rid="ft0001" ref-type="author-notes">*</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Guo</surname>
            <given-names>Fan</given-names>
          </name>
          <xref rid="aff0001" ref-type="aff">
<sup>1</sup>
</xref>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Wang</surname>
            <given-names>Huaning</given-names>
          </name>
          <xref rid="aff0002" ref-type="aff">
<sup>2</sup>
</xref>
          <xref rid="an0002" ref-type="corresp"/>
        </contrib>
        <contrib contrib-type="author">
          <name>
            <surname>Zheng</surname>
            <given-names>Minwen</given-names>
          </name>
          <xref rid="aff0001" ref-type="aff">
<sup>1</sup>
</xref>
          <xref rid="an0001" ref-type="corresp"/>
        </contrib>
        <aff id="aff0001"><label>1</label><institution>Department of Radiology, Xijing Hospital, Fourth Military Medical University</institution>, <addr-line>Xi’an</addr-line>, <addr-line>Shaanxi</addr-line>, <addr-line>710032</addr-line>, <country>People’s Republic of China</country></aff>
        <aff id="aff0002"><label>2</label><institution>Department of Psychiatry, Xijing Hospital, Fourth Military Medical University</institution>, <addr-line>Xi’an</addr-line>, <addr-line>Shaanxi</addr-line>, <addr-line>710032</addr-line>, <country>People’s Republic of China</country></aff>
      </contrib-group>
      <author-notes>
        <corresp id="an0001">Correspondence: Minwen Zheng, <institution>Department of Radiology, Xijing Hospital, Fourth Military Medical University</institution>, <addr-line>127# Changle West Road</addr-line>, <addr-line>Xi’an</addr-line>
<addr-line>710032</addr-line>, <country>People’s Republic of China</country>, Email zhengmw2007@163.com</corresp>
        <corresp id="an0002">Huaning Wang, <institution>Department of Psychiatry, Xijing Hospital, Fourth Military Medical University</institution>, <addr-line>127# Changle West Road</addr-line>, <addr-line>Xi’an</addr-line>
<addr-line>710032</addr-line>, <country>People’s Republic of China</country>, Email xskzhu@fmmu.edu.cn</corresp>
        <fn id="ft0001">
          <label>*</label>
          <p><citation_analysis attribute="neither">These authors contributed equally to this work</citation_analysis></p>
        </fn>
      </author-notes>
      <pub-date pub-type="epub">
        <day>24</day>
        <month>5</month>
        <year>2022</year>
      </pub-date>
      <pub-date pub-type="collection">
        <year>2022</year>
      </pub-date>
      <volume>14</volume>
      <fpage>995</fpage>
      <lpage>1007</lpage>
      <history>
        <date date-type="received">
          <day>24</day>
          <month>12</month>
          <year>2021</year>
        </date>
        <date date-type="accepted">
          <day>16</day>
          <month>5</month>
          <year>2022</year>
        </date>
      </history>
      <permissions>
        <copyright-statement>© 2022 Xu et al.</copyright-statement>
        <copyright-year>2022</copyright-year>
        <copyright-holder>Xu et al.</copyright-holder>
        <license>
          <ali:license_ref xmlns:ali="http://www.niso.org/schemas/ali/1.0/" specific-use="textmining" content-type="ccbynclicense">https://creativecommons.org/licenses/by-nc/3.0/</ali:license_ref>
          <license-p>This work is published and licensed by Dove Medical Press Limited. The full terms of this license are available at <ext-link ext-link-type="uri" xlink:href="https://www.dovepress.com/terms.php">https://www.dovepress.com/terms.php</ext-link> and incorporate the Creative Commons Attribution – Non Commercial (unported, v3.0) License (<ext-link ext-link-type="uri" xlink:href="https://creativecommons.org/licenses/by-nc/3.0/">http://creativecommons.org/licenses/by-nc/3.0/</ext-link>). By accessing the work you hereby accept the Terms. Non-commercial uses of the work are permitted without any further permission from Dove Medical Press Limited, provided the work is properly attributed. For permission for commercial use of this work, please see paragraphs 4.2 and 5 of our Terms (<ext-link ext-link-type="uri" xlink:href="https://www.dovepress.com/terms.php">https://www.dovepress.com/terms.php</ext-link>).</license-p>
        </license>
      </permissions>
      <abstract>
        <sec id="s2001">
          <title>Purpose</title>
          <p><citation_analysis attribute="positive">To investigate the general sleep stage classification performance of deep learning networks, three datasets, across different age groups, mental health conditions, and acquisition devices, comprising adults (SHHS) and children without mental health conditions (CCSHS), and subjects with mental health conditions (XJ), were included in this study.</citation_analysis></p>
        </sec>
        <sec id="s2002">
          <title>Methods</title>
          <p><citation_analysis attribute="neither">A long short-term memory (LSTM) network was used to evaluate the effect of different ages, mental health conditions, and acquisition devices on the sleep stage classification performance and the general performance.</citation_analysis></p>
        </sec>
        <sec id="s2003">
          <title>Results</title>
          <p><citation_analysis attribute="positive">Results showed that the age and different mental health conditions may affect the sleep stage classification performance of the network. The same acquisition device using different parameters may not have an obvious effect on the classification performance. When using a single dataset and two datasets for training, the network performed better only on the training dataset. When training was conducted with three datasets, the network performed well for all datasets with a Cohen’s Kappa of 0.8192 and 0.8472 for the SHHS and CCSHS, respectively, but performed relatively worse (0.6491) for the XJ, which indicated the complexity effect of different mental health conditions on the sleep stage classification task. Moreover, the performance of the network trained using three datasets was similar for each dataset to that of the network trained using a single dataset and tested on the same dataset.</citation_analysis></p>
        </sec>
        <sec id="s2004">
          <title>Conclusion</title>
          <p><citation_analysis attribute="positive">These results suggested that when more datasets across different age groups, mental health conditions, and acquisition devices (ie, more datasets with different feature distributions for each sleep stage) are used for training, the general performance of a deep learning network will be superior for sleep stage classification tasks with varied conditions.</citation_analysis></p>
        </sec>
      </abstract>
      <kwd-group kwd-group-type="author">
        <title>Keywords</title>
        <kwd>sleep stage classification</kwd>
        <kwd>deep learning network</kwd>
        <kwd>electroencephalogram</kwd>
        <kwd>time-frequency spectrum</kwd>
      </kwd-group>
      <funding-group>
        <award-group>
          <funding-source>
<institution-wrap><institution>National Natural Science Foundation of China</institution><institution-id institution-id-type="open-funder-registry">10.13039/501100001809</institution-id></institution-wrap>
</funding-source>
        </award-group>
        <funding-statement>This study was financially supported by the National Natural Science Foundation of China under grant 81801772 and 82071917.</funding-statement>
      </funding-group>
      <counts>
        <fig-count count="2"/>
        <table-count count="27"/>
        <ref-count count="34"/>
        <page-count count="13"/>
      </counts>
    </article-meta>
  </front>
  <body>
    <sec sec-type="intro" id="s0001">
      <title>Introduction</title>
      <p><citation_analysis attribute="neither">Sleep is an important element of one’s daily routine and occupies about one-third of the average human’s life.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0001" ref-type="bibr">1</xref></citation_analysis><citation_analysis attribute="positive"> However, a substantial number of people get inadequate sleep and have various sleep disorders, as the result of stressful life-events, the 24/7 rhythm of the modern world, shift-work, entertainment, and media consumption before sleep.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0002" ref-type="bibr">2</xref></citation_analysis><citation_analysis attribute="neither"> Sleep disturbances are a hallmark of most, if not all, neurological diseases, and can contribute to a range of health problems including obesity, depression, and anxiety, and so on.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0003" ref-type="bibr">3</xref></citation_analysis></p>
      <p><citation_analysis attribute="neither">The American Academy of Sleep Medicine (AASM) proposed that sleep can be split into five stages: the wake stage, three non-rapid eye movement sleep stages (N1 stage, N2 stage, and N3 stage), and the rapid eye movement (REM) stage. These stages are distinguished according to the amplitude, frequency, or other characteristics of neural electrical signals acquired via electroencephalography (EEG) or polysomnography (PSG) during sleep.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0004" ref-type="bibr">4</xref></citation_analysis><citation_analysis attribute="positive"> Many studies have demonstrated that the features of these electrical signals extracted from different sleep stages, such as frequency, amplitude, duration, and specific waves, and so on, are strongly associated with attention,</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0005" ref-type="bibr">5</xref></citation_analysis><citation_analysis attribute="neither">,</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0006" ref-type="bibr">6</xref></citation_analysis><citation_analysis attribute="neither"> memory,</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0007" ref-type="bibr">7</xref></citation_analysis><citation_analysis attribute="neither"> and cognition.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0008" ref-type="bibr">8</xref></citation_analysis><citation_analysis attribute="neither"> As a result, these factors have been widely used to examine the mechanisms of mental and neurological disorders such as Parkinson’s disease,</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0009" ref-type="bibr">9</xref></citation_analysis><citation_analysis attribute="neither"> Alzheimer’s disease,</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0010" ref-type="bibr">10</xref></citation_analysis><citation_analysis attribute="neither"> and schizophrenia.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0011" ref-type="bibr">11</xref></citation_analysis><citation_analysis attribute="neither">,</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0012" ref-type="bibr">12</xref></citation_analysis></p>
      <p><citation_analysis attribute="neither">To ensure the precision of these specific sleep features, accurate sleep stage classification is very important. However, as a whole night sleep often contains about 6–8 hours EEG or PSG data, manual sleep stage classification is very time consuming, and physicians may make inter- or intra-observer errors due to the fatigue and the difference in experience.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0013" ref-type="bibr">13</xref></citation_analysis><citation_analysis attribute="neither"> Currently, new technological developments have enabled researchers to propose methods for automatic sleep stage classification.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0014" ref-type="bibr">14–16</xref></citation_analysis><citation_analysis attribute="neither"> Particularly, many automatic sleep stage classification methods based on artificial intelligence (AI) have been proposed. For instance, Biswal et al proposed a novel SLEEPNET framework for sleep stage classification, consisting of a feature extraction mode, sleep stage classification mode, performance evaluation mode, and deployment mode. This system achieved an accuracy of 0.8576 and a Cohen’s Kappa of 0.7946 for the dataset created at Massachusetts General Hospital Sleep Laboratory.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0017" ref-type="bibr">17</xref></citation_analysis><citation_analysis attribute="neither"> Sun et al proposed a hierarchical neural network for automatic sleep stage classification comprising a feature extraction network and a recurrent neural network. This system had an accuracy of 0.878 and an F1-score of 0.818 on the Montreal Archive of Sleep Studies database.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0018" ref-type="bibr">18</xref></citation_analysis><citation_analysis attribute="neither"> Also, our team conducted a previous study in which we proposed an automatic sleep stage classification method based on a long short-term memory (LSTM) network that used time-frequency spectra extracted from sleep EEG signals as input. When we used our proposed method to estimate sleep stages given data from the Sleep and Heart Health Study dataset, it achieved an accuracy of 0.876 and a Cohen’s Kappa of 0.8256.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0019" ref-type="bibr">19</xref></citation_analysis><citation_analysis attribute="positive"> Although many promising AI-based automatic sleep stage classification methods have been proposed, due to the different scoring patterns by different physicians/clinics and lacking clinical certification and transparency, these methods are not yet widely used in clinical practice. But all these problems can be summarized into one main problem, namely that these studies' networks were trained using only a single dataset, which limited their application in terms of general sleep stage classification.</citation_analysis></p>
      <p><citation_analysis attribute="positive">Recently, joint training is now being conducted using multi-center datasets. For instance, Patanaik et al proposed an end-to-end convolutional neural network-based sleep stage classification method that was trained and tested using four datasets. The network had good performance for two of the training datasets and sub-optimal performance for the other two datasets.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0020" ref-type="bibr">20</xref></citation_analysis><citation_analysis attribute="neither"> Other researchers have also proposed multi-center dataset-based automatic sleep stage classification systems.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0021" ref-type="bibr">21</xref></citation_analysis><citation_analysis attribute="neither">,</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0022" ref-type="bibr">22</xref></citation_analysis><citation_analysis attribute="neither"> In 2021, Perslev et al used 23 datasets from 16 clinical studies to investigate the sleep stage classification performance of U-sleep net.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0023" ref-type="bibr">23</xref></citation_analysis><citation_analysis attribute="neither"> Although 23 datasets were used, their study did not investigate the relationship between the number of datasets used for training and the general classification performance of the network. Thus, to systematically investigate this relationship, Olesen et al used five big public datasets to conduct joint training.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0024" ref-type="bibr">24</xref></citation_analysis><citation_analysis attribute="neither"> They found that the general sleep stage classification performance was better when more datasets were used for training. To the best of our knowledge, these two studies are currently the largest investigation of automatic sleep stage classification in terms of data size and diversity. However, almost all of the subjects in these datasets were normal subjects with sleep disorders. Thus, the general performance is still unverified based on different groups of subjects. Stephansen et al used sleep stage scoring to diagnose narcolepsy,</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0021" ref-type="bibr">21</xref></citation_analysis><citation_analysis attribute="positive"> and found evidence of differences among sleep stages between healthy subjects and those with mental health conditions. Moreover, factors such as age and acquisition device may also affect sleep features.</citation_analysis></p>
      <p><citation_analysis attribute="neither">To address this, in the present study, we used three datasets, across different age groups, mental health conditions, and acquisition devices, to train a deep learning network. A LSTM network, which has a relatively simple network structure and a smaller number of training parameters compared to other types of deep learning networks,</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0025" ref-type="bibr">25</xref></citation_analysis><citation_analysis attribute="neither"> and which can consider the time information among signals, was used to perform the sleep stage classification task.</citation_analysis></p>
    </sec>
    <sec id="s0002">
      <title>Methods</title>
      <sec id="s0002-s2001">
        <title>Datasets</title>
        <p><citation_analysis attribute="neither">Three datasets were used in this study. The first two datasets were from the Sleep Heart Health Study (SHHS)</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0026" ref-type="bibr">26</xref></citation_analysis><citation_analysis attribute="neither">,</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0027" ref-type="bibr">27</xref></citation_analysis><citation_analysis attribute="neither"> and the Cleveland Children’s Sleep and Health Study (CCSHS),</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0026" ref-type="bibr">26</xref></citation_analysis><citation_analysis attribute="neither">,</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0028" ref-type="bibr">28</xref></citation_analysis><citation_analysis attribute="neither"> and were accessed via the National Sleep Research Resource (NSRR) database, which contains PSG data collected from subjects with sleep disorders and children with normal sleep, respectively. The SHHS is a large dataset with two subsets, named SHHS visit 1 (SHHS1) and SHHS visit 2 (SHHS2). The SHHS1 comprises PSG data from one night of sleep in 5793 subjects (aged 40–89) recruited between 1995 and 1998. The SHHS2 comprises PSG data from one night of sleep from 2651 of the subjects from the SHHS1, collected between 2001 and 2003. The CCSHS is a relatively small dataset, and contains PSG data from one night of sleep in 515 children (aged 16–19). The third dataset (XJ dataset) contains PSG data collected from patients with mental health conditions (depression, schizophrenia, bipolar affective disorder, and so on) in the department of psychiatry at Xijing Hospital. Specifically, the XJ comprises PSG data from one night of sleep in 8325 patients (aged 6–91) between 2010 and 2018.</citation_analysis></p>
        <p><citation_analysis attribute="neither">We split each dataset into a training and testing group according to the acquisition device and parameters. For the SHHS dataset, as SHHS2 was a second visit of SHHS1, and the acquisition parameters had some differences between two datasets, data from the SHHS1 and SHHS2 subsets were used for training and testing, respectively. For the XJ datasets, considering the updating of software system and the acquisition device, we used the data from the XJ dataset collected between 2010 and 2016 for training and the rest of the data for testing. For the CCSHS dataset, as the acquisition device and parameters were all the same, according to the training to testing ratio of SHHS and XJ datasets (about 2:1), the data from the first 360 subjects were used for training and the other data were used for testing.</citation_analysis></p>
        <p><citation_analysis attribute="positive">We re-split the training and testing groups in the SHHS and XJ datasets into several subsets according to age. For the SHHS dataset, three subsets were created: the 40–60 years group, 60–80 years group, and &gt;80 years group. For the XJ dataset, four subsets were created: the 6–20 years group, 20–40 years group, 40–60 years group, and 60–80 years group. As the number of subjects in the &gt;80 group in the XJ dataset was only 5 (training + testing), these subjects were not used for further analysis of the effects of age on sleep stage classification. As all of the subjects in the CCSHS dataset were aged 16–19 years, this dataset was not re-split.</citation_analysis></p>
        <p><citation_analysis attribute="neither">Additionally, we also re-split the training and testing groups in the XJ dataset into several subsets according to mental health conditions. Considering the number of training samples, the subsets of the training group with at least 100 subjects were used for further analysis. Finally, four subsets were created: the anxiety group, bipolar affective disease group, depression group, and schizophrenia group.</citation_analysis></p>
        <p><citation_analysis attribute="neither">The SHHS and CCSHS datasets can be requested from the NSRR (</citation_analysis><citation_analysis attribute="positive"><underline><ext-link xlink:href="https://www.sleepdata.org/" ext-link-type="uri">https://www.sleepdata.org/</ext-link></underline></citation_analysis><citation_analysis attribute="neither">). The use of the XJ dataset in this study was approved by the institutional review board of Xijing Hospital, which is affiliated with the Fourth Military Medical University.</citation_analysis></p>
      </sec>
      <sec id="s0002-s2002">
        <title>PSG Data Pre-Processing</title>
        <p><citation_analysis attribute="positive">PSG data pre-processing was performed using methods similar to those used in our previous study.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0019" ref-type="bibr">19</xref></citation_analysis><citation_analysis attribute="neither"> Specifically, a channel check was first performed for five PSG signal channels, including two EEG channels (C3 and C4), two EOG channels (left and right), and one EMG channel, to assess potential electrode dropping problems. The threshold was set at the half of the maximum acquisition amplitude for each channel, which could be acquired from the header of the EDF files. If the mean absolute signal value of a channel was bigger than the threshold, this channel was considered to have an electrode dropping problem, and the data from this channel were not used for further analysis. Considering that the electrode dropping problem is common in realistic sleeping situations, we excluded a subject if two EEG channels, two EOG channels, or the EMG channel were dropped.</citation_analysis></p>
        <p><citation_analysis attribute="neither">After the channel check, we performed zero-phase band-pass filtering. The filtering frequency was set at 0.3–45 Hz, 0.3–12 Hz, and 0.3–20 Hz for the EEG, EOG, and EMG channels, respectively. A 50th order Hamming window-based finite impulse response was used. Finally, all filtered channels were resampled to 100 Hz.</citation_analysis></p>
      </sec>
      <sec id="s0002-s2003">
        <title>Feature Extraction</title>
        <p><citation_analysis attribute="neither">We used short-time Fourier transform (STFT) to extract the time-frequency (TF) spectra from the pre-processed PSG data,</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0020" ref-type="bibr">20</xref></citation_analysis><citation_analysis attribute="neither"> which finally resulted in a </citation_analysis><citation_analysis attribute="positive"><inline-formula id="ilm0001"><alternatives><inline-graphic xlink:href="NSS-14-995-e0001.jpg"/><tex-math id="Tex001">\documentclass[12pt]{minimal}
\usepackage{wasysym}
\usepackage[substack]{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage[mathscr]{eucal}
\usepackage{mathrsfs}
\DeclareFontFamily{T1}{linotext}{}
\DeclareFontShape{T1}{linotext}{m}{n} {linotext }{}
\DeclareSymbolFont{linotext}{T1}{linotext}{m}{n}
\DeclareSymbolFontAlphabet{\mathLINOTEXT}{linotext}
\begin{document}
$$32 \times 32$$
\end{document}</tex-math></alternatives></inline-formula></citation_analysis><citation_analysis attribute="neither"> TF spectrum for each 30-s signal block. The amplitude of the frequency at each time point in the TF spectra was normalized into 0 to 1.</citation_analysis></p>
        <p><citation_analysis attribute="positive">TF spectra from four channels, named EEG, EOG L, EOG R, and EMG, were used as the input of the network. For the EEG channel, the mean of the signal from the C3 and C4 was used for TF spectra calculation. If one of these two channels had an electrode dropping problem, the signal from the normal one was used. For the EOG L and EOG R channel, if one of these two channels had an electrode dropping problem, the TF spectra of these two channels were all calculated using the signal from the normal one.</citation_analysis></p>
        <p><citation_analysis attribute="positive">According to the sleeping stage scoring standards of the AASM, there should not be any overlap between each consecutive 30-s signal block. However, this would result in a loss of information. Thus, we made a 30-s block every 10 s (ie, there was a 20-s overlap between each consecutive 30-s signal block), which might address the problem of lost information to some extent. The value of 10s was chosen for two reasons. The first is that an insufficient value will introduce large-scale redundancy to the training sample, with may lead to network overfitting. The second is that we wanted to expand the size of the training sample to be as large as possible.</citation_analysis></p>
      </sec>
      <sec id="s0002-s2004">
        <title>Classification Model</title>
        <p><citation_analysis attribute="neither">Considering the relatively simple network structure and a smaller number of trainable parameters compared to other types of deep learning networks, such AlexNet,</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0029" ref-type="bibr">29</xref></citation_analysis><citation_analysis attribute="neither"> VGG,</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0030" ref-type="bibr">30</xref></citation_analysis><citation_analysis attribute="neither"> GoogLeNet,</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0031" ref-type="bibr">31</xref></citation_analysis><citation_analysis attribute="neither"> or ResNet,</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0032" ref-type="bibr">32</xref></citation_analysis><citation_analysis attribute="neither"> the LSTM network that could also capture time information among the TF spectra was used to build the sleep stage classification model. Instead of decreasing the number of output nodes in the LSTM from 512 to 128 (eg, one LSTM hidden layer with 128 output nodes), other model settings were similar to that in our previous study.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0019" ref-type="bibr">19</xref></citation_analysis><citation_analysis attribute="neither"> Specifically, we used three TF spectra with no overlap as an input. This kind of input could consider the time information of the previous 30 s, current 30 s, and subsequent 30 s in the TF spectrum and was used to predict the sleep stage of the current time point. We also combined the TF spectra from the four channels into one big TF spectrum along with the frequency axis. Thus, the size of the TF spectra that was inputted to the LSTM network was </citation_analysis><citation_analysis attribute="positive"><inline-formula id="ilm0002"><alternatives><inline-graphic xlink:href="NSS-14-995-e0002.jpg"/><tex-math id="Tex002">\documentclass[12pt]{minimal}
\usepackage{wasysym}
\usepackage[substack]{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage[mathscr]{eucal}
\usepackage{mathrsfs}
\DeclareFontFamily{T1}{linotext}{}
\DeclareFontShape{T1}{linotext}{m}{n} {linotext }{}
\DeclareSymbolFont{linotext}{T1}{linotext}{m}{n}
\DeclareSymbolFontAlphabet{\mathLINOTEXT}{linotext}
\begin{document}
$$96 \times 128$$
\end{document}</tex-math></alternatives></inline-formula></citation_analysis><citation_analysis attribute="neither">.</citation_analysis></p>
        <p><citation_analysis attribute="positive">To evaluate the effects of age on sleep stage classification, we used each of the age subsets (3 subsets for SHHS, 1 for CCSHS, and 4 for XJ) to train the network in turn, and tested the network for all age subsets. As the three datasets were generated using different devices for data acquisition, we were also able to analyze the effects of device on sleep stage classification performance. To examine the effects of mental health conditions, we used each mental health condition subset in the XJ dataset in turn for network training, and tested the networks for all mental health condition subsets, and in the SHHS and CCSHS datasets.</citation_analysis></p>
        <p><citation_analysis attribute="positive">Moreover, we used three training strategies to systematically evaluate the general sleep stage classification performance of the LSTM network: 1) training the model using each dataset individually; 2) training the model using two datasets (3 groups, </citation_analysis><citation_analysis attribute="positive"><inline-formula id="ilm0003"><alternatives><inline-graphic xlink:href="NSS-14-995-e0003.jpg"/><tex-math id="Tex003">\documentclass[12pt]{minimal}
\usepackage{wasysym}
\usepackage[substack]{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage[mathscr]{eucal}
\usepackage{mathrsfs}
\DeclareFontFamily{T1}{linotext}{}
\DeclareFontShape{T1}{linotext}{m}{n} {linotext }{}
\DeclareSymbolFont{linotext}{T1}{linotext}{m}{n}
\DeclareSymbolFontAlphabet{\mathLINOTEXT}{linotext}
\begin{document}
$${\rm{C}}_3^2$$
\end{document}</tex-math></alternatives></inline-formula></citation_analysis><citation_analysis attribute="positive">); and 3) training the model using all three datasets. The workflow of this study is displayed in </citation_analysis><citation_analysis attribute="positive"><xref rid="f0001" ref-type="fig">Figure 1</xref></citation_analysis><citation_analysis attribute="neither">.</citation_analysis><citation_analysis attribute="positive"><fig position="float" id="f0001" fig-type="figure"><label>Figure 1</label><caption><p><citation_analysis attribute="neither">The workflow of this study. We first made a 30-s block every 10 s (ie, there was a 20-s overlap between each consecutive 30-s signal block). Thus, each 30 s EEG data had three identification points. Next, for each identification point, we used a 1 min 30 s TF spectrum as the input of LSTM network, which could consider the time information of the previous 30 s, current 30 s, and subsequent 30 s in the TF spectrum (eg, three TF spectrum blocks). Note that these three input TF spectrum blocks had no overlap between each other. Finally, the sleep stage with the maximum occurrence time was considered to be the stage of that 30-s data.</citation_analysis></p></caption><graphic xlink:href="NSS-14-995-g0001" content-type="print-only" position="float"/></fig></citation_analysis></p>
      </sec>
      <sec id="s0002-s2005">
        <title>Experimental Setup</title>
        <p><citation_analysis attribute="neither">We approached sleep stage classification as a five-class classification problem, with the following classes: 0 for the waking state, 1 for the N1 stage, 2 for the N2 stage, 3 for the N3 stage, and 4 for REM sleep. As we calculated a 30-s TF spectrum block every 10 s, each 30-s PSG data had three identification points (0 s, 10 s, and 20 s). The sleep stage with the maximum occurrence time was considered to be the stage of that 30-s data. If a 30-s PSG data had three different stages predicted by the LSTM network, we considered the stage that had the maximum predictive probability to be the stage of this 30-s PSG data.</citation_analysis></p>
        <p><citation_analysis attribute="positive">For model training, we used an Adam optimizer with 0.9 and 0.999 exponential decay rates for the first and second moments, respectively, for updating the model weights. Considering the huge number of training samples, we randomly chose about a quarter of the training samples to train the model in every epoch. The performance of the model trained using this strategy was similar to that of the model trained using all of the training samples in every epoch, and this strategy could save a lot of training time. Considering the unbalanced number of samples in each sleep stage, we used a weighted softmax cross entropy logit function as the cost function for each network.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0033" ref-type="bibr">33</xref></citation_analysis><citation_analysis attribute="positive"> The maximum number of epochs was set to 200. The initial learning rate was set at 0.001, and this was divided by 10 every 20 epochs. Training was stopped under the following conditions: 1) if the number of epochs reached the maximum number; 2) if the accuracy of the testing group decreased across five successive epochs; 3) if an absolute accuracy difference of less than 1e-5 occurred between two successive testing epochs on five successive occasions. In total, the LSTM network had 132,229 trainable parameters. To further avoid the effects of random parameter initialization on performance, we trained each LSTM network 5 times, and chose the best performance as the final performance for each model.</citation_analysis></p>
        <p><citation_analysis attribute="positive">Data pre- and post-processing (10-s strategy) and the creation of input signal blocks were implemented using MATLAB software (</citation_analysis><citation_analysis attribute="positive"><underline><ext-link xlink:href="https://www.mathworks.com/" ext-link-type="uri">https://www.mathworks.com/</ext-link></underline></citation_analysis><citation_analysis attribute="positive">). Training and testing of the LSTM network was implemented using the Tensorflow package with GPU support on a Python 3.5 platform (</citation_analysis><citation_analysis attribute="positive"><underline><ext-link xlink:href="https://www.python.org/" ext-link-type="uri">https://www.python.org/</ext-link></underline></citation_analysis><citation_analysis attribute="neither">).</citation_analysis></p>
      </sec>
      <sec id="s0002-s2006">
        <title>Evaluation Index</title>
        <p><citation_analysis attribute="neither">We used the model accuracy and Cohen’s Kappa (CK) coefficient to evaluate the performance of our LSTM-based sleep stage classification model. We used the accuracy index to assess the overall sleep stage classification precision of the model, and the CK coefficient to assess the overall performance of the model, ie, higher classification precision in each stage was associated with a higher CK coefficient. Detailed definitions of these two indices are as follows:
</citation_analysis><citation_analysis attribute="positive"><disp-formula-group><disp-formula id="m0001"><label>(1)</label><alternatives><graphic xlink:href="NSS-14-995-e0004.jpg" position="float"/><tex-math id="Tex004">\documentclass[12pt]{minimal}
\usepackage{wasysym}
\usepackage[substack]{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage[mathscr]{eucal}
\usepackage{mathrsfs}
\DeclareFontFamily{T1}{linotext}{}
\DeclareFontShape{T1}{linotext}{m}{n} {linotext }{}
\DeclareSymbolFont{linotext}{T1}{linotext}{m}{n}
\DeclareSymbolFontAlphabet{\mathLINOTEXT}{linotext}
\begin{document}
$${\rm{Accuracy}} = {{{\rm{number\ of\ accurately\ scored\ samples}}} \over {{\rm{total\ number\ of\ samples}}}}$$
\end{document}</tex-math></alternatives></disp-formula></disp-formula-group></citation_analysis><citation_analysis attribute="positive">
</citation_analysis><citation_analysis attribute="positive"><disp-formula-group><disp-formula id="m0002"><label>(2)</label><alternatives><graphic xlink:href="NSS-14-995-e0005.jpg" position="float"/><tex-math id="Tex005">\documentclass[12pt]{minimal}
\usepackage{wasysym}
\usepackage[substack]{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage[mathscr]{eucal}
\usepackage{mathrsfs}
\DeclareFontFamily{T1}{linotext}{}
\DeclareFontShape{T1}{linotext}{m}{n} {linotext }{}
\DeclareSymbolFont{linotext}{T1}{linotext}{m}{n}
\DeclareSymbolFontAlphabet{\mathLINOTEXT}{linotext}
\begin{document}
$${\rm{CK}} = {{{\rm{Accuracy}} - {P_e}} \over {1 - {P_e}}}$$
\end{document}</tex-math></alternatives></disp-formula></disp-formula-group></citation_analysis><citation_analysis attribute="positive">
</citation_analysis><citation_analysis attribute="positive"><disp-formula-group><disp-formula id="m0003"><label>(3)</label><alternatives><graphic xlink:href="NSS-14-995-e0006.jpg" position="float"/><tex-math id="Tex006">\documentclass[12pt]{minimal}
\usepackage{wasysym}
\usepackage[substack]{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage[mathscr]{eucal}
\usepackage{mathrsfs}
\DeclareFontFamily{T1}{linotext}{}
\DeclareFontShape{T1}{linotext}{m}{n} {linotext }{}
\DeclareSymbolFont{linotext}{T1}{linotext}{m}{n}
\DeclareSymbolFontAlphabet{\mathLINOTEXT}{linotext}
\begin{document}
$${P_e} = {{\sum\nolimits_i {{a_i}{b_i}} } \over {nu{m^2}}}$$
\end{document}</tex-math></alternatives></disp-formula></disp-formula-group></citation_analysis></p>
        <p><citation_analysis attribute="neither">where </citation_analysis><citation_analysis attribute="positive"><inline-formula id="ilm0004"><alternatives><inline-graphic xlink:href="NSS-14-995-e0007.jpg"/><tex-math id="Tex007">\documentclass[12pt]{minimal}
\usepackage{wasysym}
\usepackage[substack]{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage[mathscr]{eucal}
\usepackage{mathrsfs}
\DeclareFontFamily{T1}{linotext}{}
\DeclareFontShape{T1}{linotext}{m}{n} {linotext }{}
\DeclareSymbolFont{linotext}{T1}{linotext}{m}{n}
\DeclareSymbolFontAlphabet{\mathLINOTEXT}{linotext}
\begin{document}
$$i \in [{\rm{wake, N1, N2, N3, REM}}]$$
\end{document}</tex-math></alternatives></inline-formula></citation_analysis><citation_analysis attribute="neither">; </citation_analysis><citation_analysis attribute="positive"><inline-formula id="ilm0005"><alternatives><inline-graphic xlink:href="NSS-14-995-e0008.jpg"/><tex-math id="Tex008">\documentclass[12pt]{minimal}
\usepackage{wasysym}
\usepackage[substack]{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage[mathscr]{eucal}
\usepackage{mathrsfs}
\DeclareFontFamily{T1}{linotext}{}
\DeclareFontShape{T1}{linotext}{m}{n} {linotext }{}
\DeclareSymbolFont{linotext}{T1}{linotext}{m}{n}
\DeclareSymbolFontAlphabet{\mathLINOTEXT}{linotext}
\begin{document}
$${a_i}$$
\end{document}</tex-math></alternatives></inline-formula></citation_analysis><citation_analysis attribute="neither"> represents the real number of samples in a sleep stage </citation_analysis><citation_analysis attribute="positive"><italic toggle="yes">i</italic></citation_analysis><citation_analysis attribute="neither">; </citation_analysis><citation_analysis attribute="positive"><inline-formula id="ilm0006"><alternatives><inline-graphic xlink:href="NSS-14-995-e0009.jpg"/><tex-math id="Tex009">\documentclass[12pt]{minimal}
\usepackage{wasysym}
\usepackage[substack]{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{amsbsy}
\usepackage[mathscr]{eucal}
\usepackage{mathrsfs}
\DeclareFontFamily{T1}{linotext}{}
\DeclareFontShape{T1}{linotext}{m}{n} {linotext }{}
\DeclareSymbolFont{linotext}{T1}{linotext}{m}{n}
\DeclareSymbolFontAlphabet{\mathLINOTEXT}{linotext}
\begin{document}
$${b_i}$$
\end{document}</tex-math></alternatives></inline-formula></citation_analysis><citation_analysis attribute="neither"> represents the number of samples predicted by the model in a sleep stage </citation_analysis><citation_analysis attribute="positive"><italic toggle="yes">i</italic></citation_analysis><citation_analysis attribute="neither">; </citation_analysis><citation_analysis attribute="positive"><italic toggle="yes">num</italic></citation_analysis><citation_analysis attribute="neither"> represents the total number of samples.</citation_analysis></p>
      </sec>
    </sec>
    <sec id="s0003">
      <title>Results</title>
      <sec id="s0003-s2001">
        <title>Data Structure for Each Dataset</title>
        <p><citation_analysis attribute="positive"><xref rid="t0001" ref-type="table">Tables 1</xref></citation_analysis><citation_analysis attribute="neither"> and </citation_analysis><citation_analysis attribute="positive"><xref rid="t0002" ref-type="table">2</xref></citation_analysis><citation_analysis attribute="positive"> show the data structures of each dataset according to age group. In the two datasets in which the subjects had no mental health conditions (SHHS for adults and CCSHS for children, </citation_analysis><citation_analysis attribute="positive"><xref rid="t0001" ref-type="table">Table 1</xref></citation_analysis><citation_analysis attribute="positive">), the total sleep time and the N3 ratio monotonically decreased with age, and the N1 ratio and N2 ratio monotonically increased with age. Although the sleep efficiency and REM ratio monotonically decreased with age in the SHHS dataset, this was not the case when the SHHS and CCSHS were combined, perhaps because of the different acquisition devices used. In the XJ dataset, the age-related trend was the same as in the SHHS and CCSHS, but the ratio of time in each sleep stage was very different (especially for N3), which may reflect the effects of mental health conditions on sleep.</citation_analysis><citation_analysis attribute="positive"><table-wrap position="float" id="t0001"><label>Table 1</label><caption><p><citation_analysis attribute="neither">The Detailed Information of SHHS and CCSHS Datasets According to Age</citation_analysis></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="1"/><th colspan="3" rowspan="1">SHHS</th><th rowspan="1" colspan="1">CCSHS</th></tr><tr><th rowspan="1" colspan="1">40–60</th><th rowspan="1" colspan="1">60–80</th><th rowspan="1" colspan="1">&gt;80</th><th rowspan="1" colspan="1">16–19</th></tr></thead><tbody><tr><td rowspan="1" colspan="1"><bold>Number</bold></td><td rowspan="1" colspan="1">2766</td><td rowspan="1" colspan="1">4502</td><td rowspan="1" colspan="1">716</td><td rowspan="1" colspan="1">515</td></tr><tr><td rowspan="1" colspan="1"><bold>Total Sleep Time (min)</bold></td><td rowspan="1" colspan="1">378.13±61.27</td><td rowspan="1" colspan="1">360.59±65.55</td><td rowspan="1" colspan="1">345.59±68.67</td><td rowspan="1" colspan="1">463.46±69.50</td></tr><tr><td rowspan="1" colspan="1"><bold>Sleep Efficiency (%)</bold></td><td rowspan="1" colspan="1">74.35±11.84</td><td rowspan="1" colspan="1">68.60±12.44</td><td rowspan="1" colspan="1">64.71±12.82</td><td rowspan="1" colspan="1">69.79±10.27</td></tr><tr><td rowspan="1" colspan="1"><bold>N1 (%)</bold></td><td rowspan="1" colspan="1">4.82±3.31</td><td rowspan="1" colspan="1">5.67±4.08</td><td rowspan="1" colspan="1">6.61±4.48</td><td rowspan="1" colspan="1">4.15±2.69</td></tr><tr><td rowspan="1" colspan="1"><bold>N2 (%)</bold></td><td rowspan="1" colspan="1">55.94±11.43</td><td rowspan="1" colspan="1">57.91±12.94</td><td rowspan="1" colspan="1">60.25±15.02</td><td rowspan="1" colspan="1">51.98±7.46</td></tr><tr><td rowspan="1" colspan="1"><bold>N3 (%)</bold></td><td rowspan="1" colspan="1">18.35±10.82</td><td rowspan="1" colspan="1">16.90±12.02</td><td rowspan="1" colspan="1">15.97±13.91</td><td rowspan="1" colspan="1">23.24±8.07</td></tr><tr><td rowspan="1" colspan="1"><bold>REM (%)</bold></td><td rowspan="1" colspan="1">20.89±6.42</td><td rowspan="1" colspan="1">19.52±6.69</td><td rowspan="1" colspan="1">17.17±7.26</td><td rowspan="1" colspan="1">20.63±5.31</td></tr></tbody></table><table-wrap-foot><fn id="tfn0001"><p><citation_analysis attribute="positive"><bold>Notes</bold></citation_analysis><citation_analysis attribute="neither">: N1, non-rapid eye movement sleep stage 1; N2, non-rapid eye movement sleep stage 2; N3, non-rapid eye movement stage 3 and stage 4.</citation_analysis></p></fn><fn id="tfn0002"><p><citation_analysis attribute="positive"><bold>Abbreviation</bold></citation_analysis><citation_analysis attribute="neither">: REM, rapid eye movement.</citation_analysis></p></fn></table-wrap-foot></table-wrap></citation_analysis><citation_analysis attribute="positive">
</citation_analysis><citation_analysis attribute="positive"><table-wrap position="float" id="t0002"><label>Table 2</label><caption><p><citation_analysis attribute="neither">The Detailed Information of XJ Datasets According to Age</citation_analysis></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="1"/><th colspan="4" rowspan="1">XJ</th></tr><tr><th rowspan="1" colspan="1">6–20</th><th rowspan="1" colspan="1">20–40</th><th rowspan="1" colspan="1">40–60</th><th rowspan="1" colspan="1">60–80</th></tr></thead><tbody><tr><td rowspan="1" colspan="1"><bold>Number</bold></td><td rowspan="1" colspan="1">974</td><td rowspan="1" colspan="1">2778</td><td rowspan="1" colspan="1">2558</td><td rowspan="1" colspan="1">812</td></tr><tr><td rowspan="1" colspan="1"><bold>Total Sleep Time (min)</bold></td><td rowspan="1" colspan="1">420.36±68.70</td><td rowspan="1" colspan="1">406.08±72.63</td><td rowspan="1" colspan="1">384.43±78.71</td><td rowspan="1" colspan="1">358.49±87.39</td></tr><tr><td rowspan="1" colspan="1"><bold>Sleep Efficiency (%)</bold></td><td rowspan="1" colspan="1">87.04±13.18</td><td rowspan="1" colspan="1">84.68±14.02</td><td rowspan="1" colspan="1">80.63±15.44</td><td rowspan="1" colspan="1">74.85±17.63</td></tr><tr><td rowspan="1" colspan="1"><bold>N1 (%)</bold></td><td rowspan="1" colspan="1">16.18±9.56</td><td rowspan="1" colspan="1">19.50±11.01</td><td rowspan="1" colspan="1">21.79±13.21</td><td rowspan="1" colspan="1">24.39±15.27</td></tr><tr><td rowspan="1" colspan="1"><bold>N2 (%)</bold></td><td rowspan="1" colspan="1">66.60±12.12</td><td rowspan="1" colspan="1">70.46±12.27</td><td rowspan="1" colspan="1">71.43±14.61</td><td rowspan="1" colspan="1">70.21±16.73</td></tr><tr><td rowspan="1" colspan="1"><bold>N3 (%)</bold></td><td rowspan="1" colspan="1">7.64±8.09</td><td rowspan="1" colspan="1">1.45±4.00</td><td rowspan="1" colspan="1">0.15±1.09</td><td rowspan="1" colspan="1">0.04±0.45</td></tr><tr><td rowspan="1" colspan="1"><bold>REM (%)</bold></td><td rowspan="1" colspan="1">9.48±6.46</td><td rowspan="1" colspan="1">8.52±6.49</td><td rowspan="1" colspan="1">6.36±5.99</td><td rowspan="1" colspan="1">5.11±5.82</td></tr></tbody></table><table-wrap-foot><fn id="tfn0003"><p><citation_analysis attribute="positive"><bold>Notes</bold></citation_analysis><citation_analysis attribute="neither">: N1, non-rapid eye movement sleep stage 1; N2, non-rapid eye movement sleep stage 2; N3, non-rapid eye movement stage 3 and stage 4.</citation_analysis></p></fn><fn id="tfn0004"><p><citation_analysis attribute="positive"><bold>Abbreviation</bold></citation_analysis><citation_analysis attribute="neither">: REM, rapid eye movement.</citation_analysis></p></fn></table-wrap-foot></table-wrap></citation_analysis><citation_analysis attribute="positive">
</citation_analysis></p>
        <p><citation_analysis attribute="positive"><xref rid="t0003" ref-type="table">Table 3</xref></citation_analysis><citation_analysis attribute="neither"> shows the data structures of each dataset according to the different mental health conditions. The age in the anxiety and depression groups was higher than those in the bipolar affective disease and schizophrenia groups. The N3 ratio and REM ratio in the anxiety and depression groups were lower than those in the bipolar affective disease and schizophrenia groups. These results indicate that anxiety and depression may be more common in middle-aged people, and that subjects with these mental conditions may not sleep deeply compared with subjects with bipolar affective disease and schizophrenia.</citation_analysis><citation_analysis attribute="positive"><table-wrap position="float" id="t0003"><label>Table 3</label><caption><p><citation_analysis attribute="neither">The Detailed Information of XJ Datasets According to Mental Condition</citation_analysis></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="1"/><th colspan="4" rowspan="1">XJ</th></tr><tr><th rowspan="1" colspan="1">AN</th><th rowspan="1" colspan="1">BAD</th><th rowspan="1" colspan="1">DE</th><th rowspan="1" colspan="1">SZ</th></tr></thead><tbody><tr><td rowspan="1" colspan="1"><bold>Number</bold></td><td rowspan="1" colspan="1">224</td><td rowspan="1" colspan="1">507</td><td rowspan="1" colspan="1">1076</td><td rowspan="1" colspan="1">699</td></tr><tr><td rowspan="1" colspan="1"><bold>Age</bold></td><td rowspan="1" colspan="1">47.13±13.64</td><td rowspan="1" colspan="1">29.86±13.95</td><td rowspan="1" colspan="1">43.91±15.59</td><td rowspan="1" colspan="1">26.22±10.02</td></tr><tr><td rowspan="1" colspan="1"><bold>Total Sleep Time (min)</bold></td><td rowspan="1" colspan="1">415.38±52.03</td><td rowspan="1" colspan="1">426.06±61.45</td><td rowspan="1" colspan="1">410.07±68.16</td><td rowspan="1" colspan="1">413.28±75.02</td></tr><tr><td rowspan="1" colspan="1"><bold>Sleep Efficiency (%)</bold></td><td rowspan="1" colspan="1">84.28±9.87</td><td rowspan="1" colspan="1">87.48±11.50</td><td rowspan="1" colspan="1">84.00±13.60</td><td rowspan="1" colspan="1">84.22±14.71</td></tr><tr><td rowspan="1" colspan="1"><bold>N1 (%)</bold></td><td rowspan="1" colspan="1">19.53±11.94</td><td rowspan="1" colspan="1">16.90±11.25</td><td rowspan="1" colspan="1">17.46±12.14</td><td rowspan="1" colspan="1">18.15±11.40</td></tr><tr><td rowspan="1" colspan="1"><bold>N2 (%)</bold></td><td rowspan="1" colspan="1">75.30±12.81</td><td rowspan="1" colspan="1">71.64±12.81</td><td rowspan="1" colspan="1">75.99±13.39</td><td rowspan="1" colspan="1">67.51±12.48</td></tr><tr><td rowspan="1" colspan="1"><bold>N3 (%)</bold></td><td rowspan="1" colspan="1">0.29±1.60</td><td rowspan="1" colspan="1">3.12±5.88</td><td rowspan="1" colspan="1">1.04±3.57</td><td rowspan="1" colspan="1">4.06±7.47</td></tr><tr><td rowspan="1" colspan="1"><bold>REM (%)</bold></td><td rowspan="1" colspan="1">4.88±5.35</td><td rowspan="1" colspan="1">8.35±6.51</td><td rowspan="1" colspan="1">5.51±5.97</td><td rowspan="1" colspan="1">10.28±6.59</td></tr></tbody></table><table-wrap-foot><fn id="tfn0005"><p><citation_analysis attribute="positive"><bold>Abbreviations</bold></citation_analysis><citation_analysis attribute="neither">: AN, anxiety; BAD, bipolar affective disease; DE, depression; SZ, schizophrenia.</citation_analysis></p></fn></table-wrap-foot></table-wrap></citation_analysis><citation_analysis attribute="positive">
</citation_analysis></p>
      </sec>
      <sec id="s0003-s2002">
        <title>The Effect of Age on Performance of the Model</title>
        <p><citation_analysis attribute="positive"><xref rid="t0004" ref-type="table">Tables 4</xref></citation_analysis><citation_analysis attribute="neither"> and </citation_analysis><citation_analysis attribute="positive"><xref rid="t0005" ref-type="table">5</xref></citation_analysis><citation_analysis attribute="positive"> show the performance of the LSTM network for different age subsets. Overall, the LSTM had good performance for subsets from the same dataset, and poorer performance for subsets from different datasets. This may have been related to the use of different acquisition devices. For the SHHS dataset, the networks trained using each age subset performed similarly for the 40–60 and 60–80 years groups, but the performance was slightly decreased for the &gt;80 group. This trend was also observed for the XJ dataset (eg, similar performance for the 20–40 and 40–60 years groups, slight decrease in performance for the 6–20 and 60–80 years groups).</citation_analysis><citation_analysis attribute="positive"><table-wrap position="float" id="t0004"><label>Table 4</label><caption><p><citation_analysis attribute="neither">The Accuracy of LSTM Network Between Different Age Subsets</citation_analysis></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th colspan="3" rowspan="1">SHHS</th><th rowspan="1" colspan="1">CCSHS</th><th colspan="4" rowspan="1">XJ</th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">40–60</th><th rowspan="1" colspan="1">60–80</th><th rowspan="1" colspan="1">&gt;80</th><th rowspan="1" colspan="1">16–19</th><th rowspan="1" colspan="1">6–20</th><th rowspan="1" colspan="1">20–40</th><th rowspan="1" colspan="1">40–60</th><th rowspan="1" colspan="1">60–80</th></tr></thead><tbody><tr><td rowspan="3" colspan="1"><bold>SHHS</bold></td><td rowspan="1" colspan="1"><bold>40–60</bold></td><td rowspan="1" colspan="1"><bold>0.8604</bold></td><td rowspan="1" colspan="1"><bold>0.8558</bold></td><td rowspan="1" colspan="1"><bold>0.8288</bold></td><td rowspan="1" colspan="1">0.5033</td><td rowspan="1" colspan="1">0.6896</td><td rowspan="1" colspan="1">0.6986</td><td rowspan="1" colspan="1">0.6729</td><td rowspan="1" colspan="1">0.6199</td></tr><tr><td rowspan="1" colspan="1"><bold>60–80</bold></td><td rowspan="1" colspan="1"><bold>0.8603</bold></td><td rowspan="1" colspan="1"><bold>0.8541</bold></td><td rowspan="1" colspan="1"><bold>0.8254</bold></td><td rowspan="1" colspan="1">0.5804</td><td rowspan="1" colspan="1">0.7008</td><td rowspan="1" colspan="1">0.6716</td><td rowspan="1" colspan="1">0.6353</td><td rowspan="1" colspan="1">0.5978</td></tr><tr><td rowspan="1" colspan="1"><bold>&gt;80</bold></td><td rowspan="1" colspan="1"><bold>0.8214</bold></td><td rowspan="1" colspan="1"><bold>0.8313</bold></td><td rowspan="1" colspan="1"><bold>0.8141</bold></td><td rowspan="1" colspan="1">0.4222</td><td rowspan="1" colspan="1">0.6707</td><td rowspan="1" colspan="1">0.6634</td><td rowspan="1" colspan="1">0.6244</td><td rowspan="1" colspan="1">0.5854</td></tr><tr><td rowspan="1" colspan="1"><bold>CCHSH</bold></td><td rowspan="1" colspan="1"><bold>16–19</bold></td><td rowspan="1" colspan="1">0.6176</td><td rowspan="1" colspan="1">0.5884</td><td rowspan="1" colspan="1">0.5753</td><td rowspan="1" colspan="1"><bold>0.8762</bold></td><td rowspan="1" colspan="1">0.4180</td><td rowspan="1" colspan="1">0.4114</td><td rowspan="1" colspan="1">0.3930</td><td rowspan="1" colspan="1">0.3631</td></tr><tr><td rowspan="4" colspan="1"><bold>XJ</bold></td><td rowspan="1" colspan="1"><bold>6–20</bold></td><td rowspan="1" colspan="1">0.5481</td><td rowspan="1" colspan="1">0.5724</td><td rowspan="1" colspan="1">0.5948</td><td rowspan="1" colspan="1">0.3215</td><td rowspan="1" colspan="1"><bold>0.7844</bold></td><td rowspan="1" colspan="1"><bold>0.7913</bold></td><td rowspan="1" colspan="1"><bold>0.7814</bold></td><td rowspan="1" colspan="1"><bold>0.7424</bold></td></tr><tr><td rowspan="1" colspan="1"><bold>20–40</bold></td><td rowspan="1" colspan="1">0.5415</td><td rowspan="1" colspan="1">0.5821</td><td rowspan="1" colspan="1">0.6090</td><td rowspan="1" colspan="1">0.4605</td><td rowspan="1" colspan="1"><bold>0.7793</bold></td><td rowspan="1" colspan="1"><bold>0.8060</bold></td><td rowspan="1" colspan="1"><bold>0.8074</bold></td><td rowspan="1" colspan="1"><bold>0.7671</bold></td></tr><tr><td rowspan="1" colspan="1"><bold>40–60</bold></td><td rowspan="1" colspan="1">0.5233</td><td rowspan="1" colspan="1">0.5453</td><td rowspan="1" colspan="1">0.5626</td><td rowspan="1" colspan="1">0.4272</td><td rowspan="1" colspan="1"><bold>0.7810</bold></td><td rowspan="1" colspan="1"><bold>0.8159</bold></td><td rowspan="1" colspan="1"><bold>0.8224</bold></td><td rowspan="1" colspan="1"><bold>0.7807</bold></td></tr><tr><td rowspan="1" colspan="1"><bold>60–80</bold></td><td rowspan="1" colspan="1">0.5389</td><td rowspan="1" colspan="1">0.5664</td><td rowspan="1" colspan="1">0.5806</td><td rowspan="1" colspan="1">0.3367</td><td rowspan="1" colspan="1"><bold>0.7430</bold></td><td rowspan="1" colspan="1"><bold>0.7964</bold></td><td rowspan="1" colspan="1"><bold>0.8105</bold></td><td rowspan="1" colspan="1"><bold>0.7759</bold></td></tr></tbody></table><table-wrap-foot><fn id="tfn0006"><p><citation_analysis attribute="positive"><bold>Note</bold></citation_analysis><citation_analysis attribute="neither">: Bold format means the network performed well on these subsets.</citation_analysis></p></fn></table-wrap-foot></table-wrap></citation_analysis><citation_analysis attribute="positive">
</citation_analysis><citation_analysis attribute="positive"><table-wrap position="float" id="t0005"><label>Table 5</label><caption><p><citation_analysis attribute="neither">The CK Coefficient of LSTM Network Between Different Age Subsets</citation_analysis></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th colspan="3" rowspan="1">SHHS</th><th rowspan="1" colspan="1">CCSHS</th><th colspan="4" rowspan="1">XJ</th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">40–60</th><th rowspan="1" colspan="1">60–80</th><th rowspan="1" colspan="1">&gt;80</th><th rowspan="1" colspan="1">16–19</th><th rowspan="1" colspan="1">6–20</th><th rowspan="1" colspan="1">20–40</th><th rowspan="1" colspan="1">40–60</th><th rowspan="1" colspan="1">60–80</th></tr></thead><tbody><tr><td rowspan="3" colspan="1"><bold>SHHS</bold></td><td rowspan="1" colspan="1"><bold>40–60</bold></td><td rowspan="1" colspan="1"><bold>0.8013</bold></td><td rowspan="1" colspan="1"><bold>0.7906</bold></td><td rowspan="1" colspan="1"><bold>0.7447</bold></td><td rowspan="1" colspan="1">0.3194</td><td rowspan="1" colspan="1">0.5021</td><td rowspan="1" colspan="1">0.5013</td><td rowspan="1" colspan="1">0.4705</td><td rowspan="1" colspan="1">0.4282</td></tr><tr><td rowspan="1" colspan="1"><bold>60–80</bold></td><td rowspan="1" colspan="1"><bold>0.8042</bold></td><td rowspan="1" colspan="1"><bold>0.7921</bold></td><td rowspan="1" colspan="1"><bold>0.7453</bold></td><td rowspan="1" colspan="1">0.4120</td><td rowspan="1" colspan="1">0.5302</td><td rowspan="1" colspan="1">0.4706</td><td rowspan="1" colspan="1">0.4221</td><td rowspan="1" colspan="1">0.3991</td></tr><tr><td rowspan="1" colspan="1"><bold>&gt;80</bold></td><td rowspan="1" colspan="1"><bold>0.7488</bold></td><td rowspan="1" colspan="1"><bold>0.7582</bold></td><td rowspan="1" colspan="1"><bold>0.7277</bold></td><td rowspan="1" colspan="1">0.2783</td><td rowspan="1" colspan="1">0.4636</td><td rowspan="1" colspan="1">0.4484</td><td rowspan="1" colspan="1">0.4103</td><td rowspan="1" colspan="1">0.3899</td></tr><tr><td rowspan="1" colspan="1"><bold>CCHSH</bold></td><td rowspan="1" colspan="1"><bold>16–19</bold></td><td rowspan="1" colspan="1">0.4494</td><td rowspan="1" colspan="1">0.3959</td><td rowspan="1" colspan="1">0.3663</td><td rowspan="1" colspan="1"><bold>0.8294</bold></td><td rowspan="1" colspan="1">0.2594</td><td rowspan="1" colspan="1">0.2225</td><td rowspan="1" colspan="1">0.1737</td><td rowspan="1" colspan="1">0.1277</td></tr><tr><td rowspan="4" colspan="1"><bold>XJ</bold></td><td rowspan="1" colspan="1"><bold>6–20</bold></td><td rowspan="1" colspan="1">0.3462</td><td rowspan="1" colspan="1">0.3719</td><td rowspan="1" colspan="1">0.3949</td><td rowspan="1" colspan="1">0.2107</td><td rowspan="1" colspan="1"><bold>0.6424</bold></td><td rowspan="1" colspan="1"><bold>0.6374</bold></td><td rowspan="1" colspan="1"><bold>0.6122</bold></td><td rowspan="1" colspan="1"><bold>0.5672</bold></td></tr><tr><td rowspan="1" colspan="1"><bold>20–40</bold></td><td rowspan="1" colspan="1">0.3318</td><td rowspan="1" colspan="1">0.3812</td><td rowspan="1" colspan="1">0.4081</td><td rowspan="1" colspan="1">0.2660</td><td rowspan="1" colspan="1"><bold>0.6233</bold></td><td rowspan="1" colspan="1"><bold>0.6536</bold></td><td rowspan="1" colspan="1"><bold>0.6505</bold></td><td rowspan="1" colspan="1"><bold>0.6072</bold></td></tr><tr><td rowspan="1" colspan="1"><bold>40–60</bold></td><td rowspan="1" colspan="1">0.3141</td><td rowspan="1" colspan="1">0.3416</td><td rowspan="1" colspan="1">0.3514</td><td rowspan="1" colspan="1">0.2434</td><td rowspan="1" colspan="1"><bold>0.6293</bold></td><td rowspan="1" colspan="1"><bold>0.6745</bold></td><td rowspan="1" colspan="1"><bold>0.6780</bold></td><td rowspan="1" colspan="1"><bold>0.6296</bold></td></tr><tr><td rowspan="1" colspan="1"><bold>60–80</bold></td><td rowspan="1" colspan="1">0.3152</td><td rowspan="1" colspan="1">0.3445</td><td rowspan="1" colspan="1">0.3494</td><td rowspan="1" colspan="1">0.1032</td><td rowspan="1" colspan="1"><bold>0.5379</bold></td><td rowspan="1" colspan="1"><bold>0.6271</bold></td><td rowspan="1" colspan="1"><bold>0.6506</bold></td><td rowspan="1" colspan="1"><bold>0.6159</bold></td></tr></tbody></table><table-wrap-foot><fn id="tfn0007"><p><citation_analysis attribute="positive"><bold>Note</bold></citation_analysis><citation_analysis attribute="neither">: Bold format means the network performed well on these subsets.</citation_analysis></p></fn></table-wrap-foot></table-wrap></citation_analysis><citation_analysis attribute="positive">
</citation_analysis></p>
      </sec>
      <sec id="s0003-s2003">
        <title>The Effect of Mental Health Condition on Performance of the Model</title>
        <p><citation_analysis attribute="positive"><xref rid="t0006" ref-type="table">Tables 6</xref></citation_analysis><citation_analysis attribute="neither"> and </citation_analysis><citation_analysis attribute="positive"><xref rid="t0007" ref-type="table">7</xref></citation_analysis><citation_analysis attribute="positive"> show the performance of the LSTM network for the different mental health condition subsets. Overall, the LSTM performed well for each mental health condition subset in the XJ dataset, but performed worse for the SHHS and CCSHS datasets. The performance of the network trained using the anxiety and bipolar affective disease subsets was similar to that of the network trained using the schizophrenia and depression subsets, respectively.</citation_analysis><citation_analysis attribute="positive"><table-wrap position="float" id="t0006"><label>Table 6</label><caption><p><citation_analysis attribute="neither">The Accuracy of LSTM Network Between Different Mental Condition Subsets</citation_analysis></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th rowspan="2" colspan="1">SHHS</th><th rowspan="2" colspan="1">CCSHS</th><th colspan="4" rowspan="1">XJ</th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">AN</th><th rowspan="1" colspan="1">BAD</th><th rowspan="1" colspan="1">DE</th><th rowspan="1" colspan="1">SZ</th></tr></thead><tbody><tr><td rowspan="4" colspan="1"><bold>XJ</bold></td><td rowspan="1" colspan="1"><bold>AN</bold></td><td rowspan="1" colspan="1">0.4927</td><td rowspan="1" colspan="1">0.2804</td><td rowspan="1" colspan="1"><bold>0.7994</bold></td><td rowspan="1" colspan="1"><bold>0.7654</bold></td><td rowspan="1" colspan="1"><bold>0.7669</bold></td><td rowspan="1" colspan="1"><bold>0.7564</bold></td></tr><tr><td rowspan="1" colspan="1"><bold>BAD</bold></td><td rowspan="1" colspan="1">0.5094</td><td rowspan="1" colspan="1">0.3739</td><td rowspan="1" colspan="1"><bold>0.8243</bold></td><td rowspan="1" colspan="1"><bold>0.7454</bold></td><td rowspan="1" colspan="1"><bold>0.7498</bold></td><td rowspan="1" colspan="1"><bold>0.7899</bold></td></tr><tr><td rowspan="1" colspan="1"><bold>DE</bold></td><td rowspan="1" colspan="1">0.4887</td><td rowspan="1" colspan="1">0.2977</td><td rowspan="1" colspan="1"><bold>0.8291</bold></td><td rowspan="1" colspan="1"><bold>0.7463</bold></td><td rowspan="1" colspan="1"><bold>0.7510</bold></td><td rowspan="1" colspan="1"><bold>0.7757</bold></td></tr><tr><td rowspan="1" colspan="1"><bold>SZ</bold></td><td rowspan="1" colspan="1">0.5386</td><td rowspan="1" colspan="1">0.2638</td><td rowspan="1" colspan="1"><bold>0.7919</bold></td><td rowspan="1" colspan="1"><bold>0.7685</bold></td><td rowspan="1" colspan="1"><bold>0.7558</bold></td><td rowspan="1" colspan="1"><bold>0.7472</bold></td></tr></tbody></table><table-wrap-foot><fn id="tfn0008"><p><citation_analysis attribute="positive"><bold>Note</bold></citation_analysis><citation_analysis attribute="neither">: Bold format means the network performed well on these subsets.</citation_analysis></p></fn><fn id="tfn0009"><p><citation_analysis attribute="positive"><bold>Abbreviations</bold></citation_analysis><citation_analysis attribute="neither">: AN, anxiety; BAD, bipolar affective disease; DE, depression; SZ, schizophrenia.</citation_analysis></p></fn></table-wrap-foot></table-wrap></citation_analysis><citation_analysis attribute="positive">
</citation_analysis><citation_analysis attribute="positive"><table-wrap position="float" id="t0007"><label>Table 7</label><caption><p><citation_analysis attribute="neither">The CK Coefficient of LSTM Network Between Different Mental Condition Subsets</citation_analysis></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th rowspan="2" colspan="1">SHHS</th><th rowspan="2" colspan="1">CCSHS</th><th colspan="4" rowspan="1">XJ</th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">AN</th><th rowspan="1" colspan="1">BAD</th><th rowspan="1" colspan="1">DE</th><th rowspan="1" colspan="1">SZ</th></tr></thead><tbody><tr><td rowspan="4" colspan="1"><bold>XJ</bold></td><td rowspan="1" colspan="1"><bold>AN</bold></td><td rowspan="1" colspan="1">0.2606</td><td rowspan="1" colspan="1">0.1279</td><td rowspan="1" colspan="1"><bold>0.6137</bold></td><td rowspan="1" colspan="1"><bold>0.5735</bold></td><td rowspan="1" colspan="1"><bold>0.5732</bold></td><td rowspan="1" colspan="1"><bold>0.5861</bold></td></tr><tr><td rowspan="1" colspan="1"><bold>BAD</bold></td><td rowspan="1" colspan="1">0.2849</td><td rowspan="1" colspan="1">0.1122</td><td rowspan="1" colspan="1"><bold>0.6321</bold></td><td rowspan="1" colspan="1"><bold>0.5283</bold></td><td rowspan="1" colspan="1"><bold>0.5205</bold></td><td rowspan="1" colspan="1"><bold>0.6396</bold></td></tr><tr><td rowspan="1" colspan="1"><bold>DE</bold></td><td rowspan="1" colspan="1">0.2717</td><td rowspan="1" colspan="1">0.1002</td><td rowspan="1" colspan="1"><bold>0.6540</bold></td><td rowspan="1" colspan="1"><bold>0.5503</bold></td><td rowspan="1" colspan="1"><bold>0.5401</bold></td><td rowspan="1" colspan="1"><bold>0.6174</bold></td></tr><tr><td rowspan="1" colspan="1"><bold>SZ</bold></td><td rowspan="1" colspan="1">0.3252</td><td rowspan="1" colspan="1">0.1269</td><td rowspan="1" colspan="1"><bold>0.5966</bold></td><td rowspan="1" colspan="1"><bold>0.5861</bold></td><td rowspan="1" colspan="1"><bold>0.5550</bold></td><td rowspan="1" colspan="1"><bold>0.5806</bold></td></tr></tbody></table><table-wrap-foot><fn id="tfn0010"><p><citation_analysis attribute="positive"><bold>Note</bold></citation_analysis><citation_analysis attribute="neither">: Bold format means the network performed well on these subsets.</citation_analysis></p></fn><fn id="tfn0011"><p><citation_analysis attribute="positive"><bold>Abbreviations</bold></citation_analysis><citation_analysis attribute="neither">: AN, anxiety; BAD, bipolar affective disease; DE, depression; SZ, schizophrenia.</citation_analysis></p></fn></table-wrap-foot></table-wrap></citation_analysis><citation_analysis attribute="positive">
</citation_analysis></p>
      </sec>
      <sec id="s0003-s2004">
        <title>General Performance of Model</title>
        <p><citation_analysis attribute="positive">When trained using a single dataset, for each dataset, the classification performance was better only when tested using the data from that dataset and poorer for the data from the other two datasets (</citation_analysis><citation_analysis attribute="positive"><xref rid="t0008" ref-type="table">Table 8</xref></citation_analysis><citation_analysis attribute="neither">). For each dataset used for training, the testing accuracy and CK coefficient were 0.8685 and 0.8115, 0.8762 and 0.8294, and 0.7902 and 0.6377 for SHHS, CCSHS, and XJ, respectively; however, for the other two datasets, the testing accuracy and CK coefficient were decreased severely.</citation_analysis><citation_analysis attribute="positive"><table-wrap position="float" id="t0008"><label>Table 8</label><caption><p><citation_analysis attribute="neither">The Performance of LSTM Network Trained Using Single Dataset</citation_analysis></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="1">Test<break/>Train</th><th rowspan="1" colspan="1"/><th colspan="2" rowspan="1">SHHS</th><th colspan="2" rowspan="1">CCSHS</th><th colspan="2" rowspan="1">XJ</th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">30 s</th><th rowspan="1" colspan="1">10 s</th><th rowspan="1" colspan="1">30 s</th><th rowspan="1" colspan="1">10 s</th><th rowspan="1" colspan="1">30 s</th><th rowspan="1" colspan="1">10 s</th></tr></thead><tbody><tr><td rowspan="2" colspan="1"><bold>SHHS</bold></td><td rowspan="1" colspan="1"><bold>ACC</bold></td><td rowspan="1" colspan="1"><bold>0.8685</bold></td><td rowspan="1" colspan="1"><bold>0.8757</bold></td><td rowspan="1" colspan="1">0.6166</td><td rowspan="1" colspan="1">0.6225</td><td rowspan="1" colspan="1">0.6691</td><td rowspan="1" colspan="1">0.6768</td></tr><tr><td rowspan="1" colspan="1"><bold>CK</bold></td><td rowspan="1" colspan="1"><bold>0.8115</bold></td><td rowspan="1" colspan="1"><bold>0.8216</bold></td><td rowspan="1" colspan="1">0.4913</td><td rowspan="1" colspan="1">0.4987</td><td rowspan="1" colspan="1">0.4735</td><td rowspan="1" colspan="1">0.4839</td></tr><tr><td rowspan="2" colspan="1"><bold>CCSHS</bold></td><td rowspan="1" colspan="1"><bold>ACC</bold></td><td rowspan="1" colspan="1">0.5932</td><td rowspan="1" colspan="1">0.6010</td><td rowspan="1" colspan="1"><bold>0.8762</bold></td><td rowspan="1" colspan="1"><bold>0.8820</bold></td><td rowspan="1" colspan="1">0.4002</td><td rowspan="1" colspan="1">0.4013</td></tr><tr><td rowspan="1" colspan="1"><bold>CK</bold></td><td rowspan="1" colspan="1">0.4050</td><td rowspan="1" colspan="1">0.4153</td><td rowspan="1" colspan="1"><bold>0.8294</bold></td><td rowspan="1" colspan="1"><bold>0.8374</bold></td><td rowspan="1" colspan="1">0.2077</td><td rowspan="1" colspan="1">0.2089</td></tr><tr><td rowspan="2" colspan="1"><bold>XJ</bold></td><td rowspan="1" colspan="1"><bold>ACC</bold></td><td rowspan="1" colspan="1">0.5956</td><td rowspan="1" colspan="1">0.5968</td><td rowspan="1" colspan="1">0.4440</td><td rowspan="1" colspan="1">0.4463</td><td rowspan="1" colspan="1"><bold>0.7902</bold></td><td rowspan="1" colspan="1"><bold>0.7958</bold></td></tr><tr><td rowspan="1" colspan="1"><bold>CK</bold></td><td rowspan="1" colspan="1">0.4072</td><td rowspan="1" colspan="1">0.4085</td><td rowspan="1" colspan="1">0.2893</td><td rowspan="1" colspan="1">0.2931</td><td rowspan="1" colspan="1"><bold>0.6377</bold></td><td rowspan="1" colspan="1"><bold>0.6467</bold></td></tr></tbody></table><table-wrap-foot><fn id="tfn0012"><p><citation_analysis attribute="positive"><bold>Notes</bold></citation_analysis><citation_analysis attribute="neither">: 10 s, predicting the sleep stage every 10 s; 30 s, predicting the sleep stage every 30 s. Bold format means the network performed well on these subsets.</citation_analysis></p></fn><fn id="tfn0013"><p><citation_analysis attribute="positive"><bold>Abbreviations</bold></citation_analysis><citation_analysis attribute="neither">: ACC, accuracy; CK, Cohen’s Kappa coefficient.</citation_analysis></p></fn></table-wrap-foot></table-wrap></citation_analysis><citation_analysis attribute="positive">
</citation_analysis></p>
        <p><citation_analysis attribute="positive">When trained using two datasets, the classification performance of the network was still only better for data from the dataset used for training (</citation_analysis><citation_analysis attribute="positive"><xref rid="t0009" ref-type="table">Table 9</xref></citation_analysis><citation_analysis attribute="neither">), which was different from the results in Olesen’s study, that the general classification performance of the network would improve with the increasing number of datasets used for training.</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0024" ref-type="bibr">24</xref></citation_analysis><citation_analysis attribute="positive"><table-wrap position="float" id="t0009"><label>Table 9</label><caption><p><citation_analysis attribute="neither">The Performance of LSTM Network Trained Using Two Datasets</citation_analysis></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="1">Test<break/>Train</th><th rowspan="1" colspan="1"/><th colspan="2" rowspan="1">SHHS</th><th colspan="2" rowspan="1">CCSHS</th><th colspan="2" rowspan="1">XJ</th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">30 s</th><th rowspan="1" colspan="1">10 s</th><th rowspan="1" colspan="1">30 s</th><th rowspan="1" colspan="1">10 s</th><th rowspan="1" colspan="1">30 s</th><th rowspan="1" colspan="1">10 s</th></tr></thead><tbody><tr><td rowspan="2" colspan="1"><bold>SHHS+CCSHS</bold></td><td rowspan="1" colspan="1"><bold>ACC</bold></td><td rowspan="1" colspan="1"><bold>0.8658</bold></td><td rowspan="1" colspan="1"><bold>0.8724</bold></td><td rowspan="1" colspan="1"><bold>0.8711</bold></td><td rowspan="1" colspan="1"><bold>0.8779</bold></td><td rowspan="1" colspan="1">0.5743</td><td rowspan="1" colspan="1">0.5778</td></tr><tr><td rowspan="1" colspan="1"><bold>CK</bold></td><td rowspan="1" colspan="1"><bold>0.8076</bold></td><td rowspan="1" colspan="1"><bold>0.8168</bold></td><td rowspan="1" colspan="1"><bold>0.8231</bold></td><td rowspan="1" colspan="1"><bold>0.8322</bold></td><td rowspan="1" colspan="1">0.3542</td><td rowspan="1" colspan="1">0.3583</td></tr><tr><td rowspan="2" colspan="1"><bold>SHHS+XJ</bold></td><td rowspan="1" colspan="1"><bold>ACC</bold></td><td rowspan="1" colspan="1"><bold>0.8722</bold></td><td rowspan="1" colspan="1"><bold>0.8791</bold></td><td rowspan="1" colspan="1">0.5783</td><td rowspan="1" colspan="1">0.5834</td><td rowspan="1" colspan="1"><bold>0.7932</bold></td><td rowspan="1" colspan="1"><bold>0.7980</bold></td></tr><tr><td rowspan="1" colspan="1"><bold>CK</bold></td><td rowspan="1" colspan="1"><bold>0.8162</bold></td><td rowspan="1" colspan="1"><bold>0.8260</bold></td><td rowspan="1" colspan="1">0.4336</td><td rowspan="1" colspan="1">0.4403</td><td rowspan="1" colspan="1"><bold>0.6412</bold></td><td rowspan="1" colspan="1"><bold>0.6500</bold></td></tr><tr><td rowspan="2" colspan="1"><bold>CCSHS+XJ</bold></td><td rowspan="1" colspan="1"><bold>ACC</bold></td><td rowspan="1" colspan="1">0.5915</td><td rowspan="1" colspan="1">0.5940</td><td rowspan="1" colspan="1"><bold>0.8579</bold></td><td rowspan="1" colspan="1"><bold>0.8629</bold></td><td rowspan="1" colspan="1"><bold>0.7830</bold></td><td rowspan="1" colspan="1"><bold>0.7881</bold></td></tr><tr><td rowspan="1" colspan="1"><bold>CK</bold></td><td rowspan="1" colspan="1">0.3947</td><td rowspan="1" colspan="1">0.3978</td><td rowspan="1" colspan="1"><bold>0.8034</bold></td><td rowspan="1" colspan="1"><bold>0.8102</bold></td><td rowspan="1" colspan="1"><bold>0.6224</bold></td><td rowspan="1" colspan="1"><bold>0.6307</bold></td></tr></tbody></table><table-wrap-foot><fn id="tfn0014"><p><citation_analysis attribute="positive"><bold>Notes</bold></citation_analysis><citation_analysis attribute="neither">: 10 s, predicting the sleep stage every 10 s; 30 s, predicting the sleep stage every 30 s. Bold format means the network performed well on these subsets.</citation_analysis></p></fn><fn id="tfn0015"><p><citation_analysis attribute="positive"><bold>Abbreviations</bold></citation_analysis><citation_analysis attribute="neither">: ACC, accuracy; CK, Cohen’s Kappa coefficient.</citation_analysis></p></fn></table-wrap-foot></table-wrap></citation_analysis><citation_analysis attribute="positive">
</citation_analysis></p>
        <p><citation_analysis attribute="positive"><xref rid="t0010" ref-type="table">Table 10</xref></citation_analysis><citation_analysis attribute="positive"> shows the sleep stage classification of the LSTM network trained using all datasets. The performance of the network was better for all datasets. The testing accuracy and CK coefficient were 0.8680 and 0.8102, 0.8825 and 0.8380, and 0.7923 and 0.6402 for SHHS, CCSHS, and XJ, respectively, which was very close to that of the network trained using a single dataset and tested on the same dataset.</citation_analysis><citation_analysis attribute="positive"><table-wrap position="float" id="t0010"><label>Table 10</label><caption><p><citation_analysis attribute="neither">The Performance of LSTM Network Trained Using All Datasets</citation_analysis></p></caption><table frame="hsides" rules="groups"><thead><tr><th rowspan="2" colspan="1">Test<break/>Train</th><th rowspan="1" colspan="1"/><th colspan="2" rowspan="1">SHHS</th><th colspan="2" rowspan="1">CCSHS</th><th colspan="2" rowspan="1">XJ</th></tr><tr><th rowspan="1" colspan="1"/><th rowspan="1" colspan="1">30 s</th><th rowspan="1" colspan="1">10 s</th><th rowspan="1" colspan="1">30 s</th><th rowspan="1" colspan="1">10 s</th><th rowspan="1" colspan="1">30 s</th><th rowspan="1" colspan="1">10 s</th></tr></thead><tbody><tr><td rowspan="2" colspan="1"><bold>SHHS+CCSHS+XJ</bold></td><td rowspan="1" colspan="1"><bold>ACC</bold></td><td rowspan="1" colspan="1">0.8680</td><td rowspan="1" colspan="1">0.8744</td><td rowspan="1" colspan="1">0.8825</td><td rowspan="1" colspan="1">0.8893</td><td rowspan="1" colspan="1">0.7923</td><td rowspan="1" colspan="1">0.7978</td></tr><tr><td rowspan="1" colspan="1"><bold>CK</bold></td><td rowspan="1" colspan="1">0.8102</td><td rowspan="1" colspan="1">0.8192</td><td rowspan="1" colspan="1">0.8380</td><td rowspan="1" colspan="1">0.8472</td><td rowspan="1" colspan="1">0.6402</td><td rowspan="1" colspan="1">0.6491</td></tr></tbody></table><table-wrap-foot><fn id="tfn0016"><p><citation_analysis attribute="positive"><bold>Notes</bold></citation_analysis><citation_analysis attribute="neither">: 10 s, predicting the sleep stage every 10 s; 30 s, predicting the sleep stage every 30 s.</citation_analysis></p></fn><fn id="tfn0017"><p><citation_analysis attribute="positive"><bold>Abbreviations</bold></citation_analysis><citation_analysis attribute="neither">: ACC, accuracy; CK, Cohen’s Kappa coefficient.</citation_analysis></p></fn></table-wrap-foot></table-wrap></citation_analysis><citation_analysis attribute="positive">
</citation_analysis></p>
      </sec>
      <sec id="s0003-s2005">
        <title>Performance of Model Using 10-s Strategy</title>
        <p><citation_analysis attribute="positive">The network general performance was evaluated using both original 30-s and 10-s strategy. From </citation_analysis><citation_analysis attribute="positive"><xref rid="t0008 t0009 t0010" ref-type="table">Tables 8–10</xref></citation_analysis><citation_analysis attribute="positive">, the 10-s strategy could only improve the sleep stage classification performance with about 0.5%. Thus, we suggested that the original 30-s strategy was enough for sleep stage classification. However, the 10-s strategy was a good way to expand the size of the training sample, because the specific sleep features or waves might appear at any time points in a 30-s window.</citation_analysis></p>
      </sec>
    </sec>
    <sec id="s0004">
      <title>Discussion</title>
      <p><citation_analysis attribute="neither">In this study, we used an LSTM network for the sleep stage classification. We used three datasets to evaluate the model classification performance between different age, acquisition device, and mental health conditions, and the general performance (SHHS and CCSHS for subjects without mental health conditions, XJ for subjects with mental health conditions). We also proposed a 10-s sleep stage identification method to correct the classification results obtained by the LSTM network. Our results showed that the network performed better on the training dataset but worse on the remaining dataset(s) when we used a single dataset or two datasets for training. In all-datasets training situations, the network performed well for all datasets. However, the age, the acquisition device and presence of mental health conditions could have led to differences in sleep features, which may have affected the sleep stage classification performance of the LSTM network. The corresponding results are discussed in detail below.</citation_analysis></p>
      <p><citation_analysis attribute="neither">From </citation_analysis><citation_analysis attribute="positive"><xref rid="t0004" ref-type="table">Tables 4</xref></citation_analysis><citation_analysis attribute="neither"> and </citation_analysis><citation_analysis attribute="positive"><xref rid="t0005" ref-type="table">5</xref></citation_analysis><citation_analysis attribute="positive">, for the SHHS dataset, networks trained using each age subset performed similarly for the 40–60 and 60–80 years groups, but performance slightly decreased for the &gt;80 group. A similar trend was found for the XJ dataset. These results suggest that sleep features may change with age. However, the relatively small difference in performance between the subsets in the same dataset indicates that the effect of age on sleep stage classification may be small. Additionally, we obtained the following interesting results. First, when testing with different datasets, the networks trained using the age subsets from the SHHS dataset performed relatively well with the XJ dataset compared with the CCSHS dataset. Second, the networks trained using the subsets from the XJ dataset performed best with the &gt;80 years group from the SHHS dataset. These results indicate that the sleep features of younger subjects with mental health conditions may be similar to those of elderly subjects with no mental health conditions.</citation_analysis></p>
      <p><citation_analysis attribute="positive">For the XJ dataset, the networks trained using each mental health condition subset performed similarly for the anxiety and schizophrenia groups and for the bipolar affective disorder and depression groups. Thus, sleep features may be similar between these two pairs of mental health conditions. We also found that networks trained using each mental health condition subset performed relatively well with the anxiety subset compared with the other subsets. This may indicate that anxiety has a relatively small effect on sleep features compared with the other mental health conditions. Still, networks trained using the mental health condition subsets performed relatively well with the SHHS dataset compared with the CCSHS dataset, which may further suggest that the sleep features of subjects with mental health conditions are similar to those of elderly subjects with no mental health conditions.</citation_analysis></p>
      <p><citation_analysis attribute="neither">As for the effect of acquisition device on sleep stage classification, we found that in datasets that used the same acquisition device, different acquisition parameters did not appear to have a strong effect on the classification task. As mentioned above, the LSTM performed well on subsets from the same dataset (age subsets in the SHHS and XJ and mental health condition subsets in the XJ, </citation_analysis><citation_analysis attribute="positive"><xref rid="t0004 t0005 t0006 t0007" ref-type="table">Tables 4–7</xref></citation_analysis><citation_analysis attribute="positive">). However, when we trained a network using one dataset and tested it using the other two datasets, the performance was dramatically decreased. Although the acquisition device was different between the three datasets, as none of them contained EEG or PSG data acquired using different devices, the decreased performance maybe not necessarily caused by the use of different acquisition devices. Indeed, different scoring patterns used by different physicians/clinics could also impact the performance. Thus, future studies with datasets containing data acquired using different devices are needed to further investigate the effects of acquisition device on sleep stage classification.</citation_analysis></p>
      <p><citation_analysis attribute="neither">When we used a single dataset to train the LSTM network, the network performed well only on the trained dataset. When the number of datasets used for training was increased to two, the classification performance of the LSTM network for the remaining dataset was similar to that observed in the single dataset training situation (</citation_analysis><citation_analysis attribute="positive"><xref rid="t0009" ref-type="table">Table 9</xref></citation_analysis><citation_analysis attribute="neither"> vs </citation_analysis><citation_analysis attribute="positive"><xref rid="t0008" ref-type="table">Table 8</xref></citation_analysis><citation_analysis attribute="neither">). The overfitting problem</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0034" ref-type="bibr">34</xref></citation_analysis><citation_analysis attribute="positive"> might explain these results. However, the network performed well with the datasets used for training, especially after joint training with the SHHS and XJ dataset (which comprised data from subjects without and with mental health conditions, respectively), and joint training with the CCSHS and XJ datasets (which were dramatically different in terms of sample size and also contained data from two different groups of subjects). These results suggested that the LSTM network indeed could identify the sleep stage features across different datasets. However, it still performed worse on the remaining dataset. Thus, we assumed that the difference of TF spectrum features for each sleep stage among three datasets might be the main reason. When two datasets were used for training, the deep learning network could only capture the information from the two training datasets, and therefore exhibited poorer performed on the remaining dataset.</citation_analysis></p>
      <p><citation_analysis attribute="positive">When we used all of the datasets to train the network, the LSTM network performed well on all three datasets. Moreover, the performance of the network trained using three datasets was similar for each dataset to that of the network trained using a single dataset and tested on the same dataset (</citation_analysis><citation_analysis attribute="positive"><xref rid="t0010" ref-type="table">Table 10</xref></citation_analysis><citation_analysis attribute="neither"> vs </citation_analysis><citation_analysis attribute="positive"><xref rid="t0008" ref-type="table">Table 8</xref></citation_analysis><citation_analysis attribute="positive">). These results further proved our assumption above. Thus, our results were partly consistent with Olesen’s finding</citation_analysis><citation_analysis attribute="positive"><xref rid="cit0024" ref-type="bibr">24</xref></citation_analysis><citation_analysis attribute="positive"> that, when the number of datasets used for network training is increased, the general classification performance of the network improves. However, this only holds when all of the datasets have similar feature distributions for each sleep stage (almost all of the subjects in Olesen’s study were subjects with sleep disorders). Thus, as a supplement to Olesen’s study, our results suggest that using more datasets across different age groups, mental health conditions, and acquisition devices and parameters for training will enhance the general performance of the deep learning network, where age and mental health conditions are relatively important. Conversely, networks trained using only datasets from subjects with similar conditions will perform poorly on datasets from subjects with new conditions. Besides, the LSTM network performed well on all datasets after joint training, which also indicates that a deep learning network has the ability to identify the sleep feature difference among subjects with different ages and mental health conditions, and therefore has great feature generalization ability.</citation_analysis></p>
      <p><citation_analysis attribute="positive">After comparing the classification sensitivity for each sleep stage among the three datasets, we found some interesting differences between models prior to correction by our proposed sleep stage identification strategy (as this strategy was a post-processing operation, the classification performance corrected by this strategy cannot represent the performance of the LSTM network itself). First, as shown in </citation_analysis><citation_analysis attribute="positive"><xref rid="f0002" ref-type="fig">Figure 2</xref></citation_analysis><citation_analysis attribute="neither">, the sensitivity to the wake, N1, N3, and REM stages in the single-dataset training situation substantially differed between the SHHS and XJ datasets. If this situation is considered in isolation, it is possible to imagine that these results might have been caused by the difference in the sample distribution in the training samples between these two datasets (</citation_analysis><citation_analysis attribute="positive"><xref rid="t0001 t0002" ref-type="table">Tables 1 and 2</xref></citation_analysis><citation_analysis attribute="positive">). During joint training, the number of training samples in each sleep stage had been extremely supplemented; however, the sensitivity to these four sleep stages was still similar to that in the single-dataset training situation, although there existed some increases or decreases in performance. Thus, these results suggest that the PSG signals associated with the wake, N1, N3, and REM sleep stages may differ substantially between subjects with (XJ) and without (SHHS) mental health conditions. Second, all of the sleep stages had similar sensitivity in the SHHS and CCSHS datasets during both single-dataset training and joint-dataset training. This means that the PSG sleep signals may have been similar between children (CCSHS) and adults (SHHS) without mental health conditions. Third, when comparing the misclassified ratio for each sleep stage among the three datasets, the N1 stage may be more clearly different from the wake and N2 stages in the subjects with mental health conditions, and the N3 and REM stages may be more similar to the N2 and the N1 stages, respectively, in these subjects. Moreover, differences in the performance of networks trained using different subsets could also suggest that sleep features vary according to experimental conditions (eg, similar performance reflects similar sleep features between conditions, and vice versa). All of these results indicate that as long as the design of a scientific problem is reasonable, a deep learning model can be used to investigate the mechanism of the problem without analyzing the “black box” of the model itself. This notion may stimulate new ideas regarding mechanism research using deep learning-based methods. However, this kind of design is also a big challenge. Sleep stage classification is a special case, and additional research will be needed to apply this type of design to other clinical problems.</citation_analysis><citation_analysis attribute="positive"><fig position="float" id="f0002" fig-type="figure"><label>Figure 2</label><caption><p><citation_analysis attribute="neither">Comparison of the sleep stage classification sensitivity between single-dataset training and joint training situations. The dialog elements of each matrix represent the classification sensitivity of the network in each sleep stage.</citation_analysis></p></caption><graphic xlink:href="NSS-14-995-g0002" content-type="print-only" position="float"/></fig></citation_analysis></p>
      <p><citation_analysis attribute="neither">This study had several limitations. First, we only used three datasets in this study. Future studies will use more datasets, especially those containing subjects with mental health conditions, to validate the results of this study and further improve the sleep stage classification performance of the LSTM network. Second, due to the sample size, we did not further split each mental health condition subset of the XJ dataset into several age subsets. Further studies will collect more data to investigate in detail the effect of age and mental health conditions on sleep stage classification.</citation_analysis></p>
      <p><citation_analysis attribute="positive">In summary, this study investigated the sleep stage classification performance of a deep learning network trained using three datasets. Our results indicate that using more datasets across different age groups, mental health conditions, and acquisition devices and parameters for training may enhance the general performance of the network for sleep stage classification tasks under varied conditions. Deep learning-based methods can also be used to investigate the mechanisms of a scientific problem without analyzing the “black box” of the model itself. However, an appropriate experimental design is needed.</citation_analysis></p>
    </sec>
  </body>
  <back>
    <ack>
      <title>Acknowledgment</title>
      <p><citation_analysis attribute="neither">We thank Sydney Koke, MFA, from Liwen Bianji (Edanz) (</citation_analysis><citation_analysis attribute="positive"><underline><ext-link xlink:href="https://www.liwenbianji.cn" ext-link-type="uri">https://www.liwenbianji.cn</ext-link></underline></citation_analysis><citation_analysis attribute="neither">), for editing the English text of a draft of this manuscript.</citation_analysis></p>
      <p><citation_analysis attribute="neither">We thank the National Sleep Research Resource (</citation_analysis><citation_analysis attribute="positive"><underline><ext-link xlink:href="https://www.sleepdata.org" ext-link-type="uri">https://www.sleepdata.org</ext-link></underline></citation_analysis><citation_analysis attribute="neither">) team for their work in sharing SHHS (</citation_analysis><citation_analysis attribute="positive"><underline><ext-link xlink:href="https://www.sleepdata.org/datasets/shhs" ext-link-type="uri">https://www.sleepdata.org/datasets/shhs</ext-link></underline></citation_analysis><citation_analysis attribute="neither">) and CCSHS datasets (</citation_analysis><citation_analysis attribute="positive"><underline><ext-link xlink:href="https://www.sleepdata.org/datasets/ccshs" ext-link-type="uri">https://www.sleepdata.org/datasets/ccshs</ext-link></underline></citation_analysis><citation_analysis attribute="neither">) used in this study.</citation_analysis></p>
    </ack>
    <sec sec-type="COI-statement" id="s0005">
      <title>Disclosure</title>
      <p><citation_analysis attribute="neither">The authors report no conflicts of interest in this work.</citation_analysis></p>
    </sec>
    <ref-list>
      <title>References</title>
      <ref id="cit0001">
        <label>1.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Cirelli</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Tononi</surname>
<given-names>G</given-names></string-name>. <article-title>Is sleep essential?</article-title>
<source><italic toggle="yes">PLoS Biol</italic></source>. <year>2008</year>;<volume>6</volume>(<issue>8</issue>):<fpage>e216</fpage>. doi:<pub-id pub-id-type="doi">10.1371/journal.pbio.0060216</pub-id><pub-id pub-id-type="pmid">18752355</pub-id></mixed-citation>
      </ref>
      <ref id="cit0002">
        <label>2.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Geiker</surname>
<given-names>NRW</given-names></string-name>, <string-name><surname>Astrup</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Hjorth</surname>
<given-names>MF</given-names></string-name>, <string-name><surname>Sjödin</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Pijls</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Markus</surname>
<given-names>CR</given-names></string-name>. <article-title>Does stress influence sleep patterns, food intake, weight gain, abdominal obesity and weight loss interventions and vice versa?</article-title>
<source><italic toggle="yes">Obes Rev</italic></source>. <year>2018</year>;<volume>19</volume>:<fpage>81</fpage>–<lpage>97</lpage>. doi:<pub-id pub-id-type="doi">10.1111/obr.12603</pub-id><pub-id pub-id-type="pmid">28849612</pub-id></mixed-citation>
      </ref>
      <ref id="cit0003">
        <label>3.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Iranzo</surname>
<given-names>A</given-names></string-name>. <article-title>Sleep and neurological autoimmune diseases</article-title>. <source><italic toggle="yes">Neuropsychopharmacology</italic></source>. <year>2020</year>;<volume>45</volume>(<issue>1</issue>):<fpage>129</fpage>–<lpage>140</lpage>. doi:<pub-id pub-id-type="doi">10.1038/s41386-019-0463-z</pub-id><pub-id pub-id-type="pmid">31302665</pub-id></mixed-citation>
      </ref>
      <ref id="cit0004">
        <label>4.</label>
        <mixed-citation publication-type="book"><string-name><surname>Berry</surname>
<given-names>RB</given-names></string-name>, <string-name><surname>Albertario</surname>
<given-names>CL</given-names></string-name>, <string-name><surname>Harding</surname>
<given-names>SM</given-names></string-name>, et al. <article-title>for the American Academy of Sleep Medicine. The AASM manual for the scoring of sleep and associated events: rules, terminology and technical specifications</article-title>. <publisher-loc>Darien, IL</publisher-loc>: <publisher-name>American Academy of Sleep Medicine</publisher-name>. <year>2018</year>.</mixed-citation>
      </ref>
      <ref id="cit0005">
        <label>5.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Furrer</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Jaramillo</surname>
<given-names>V</given-names></string-name>, <string-name><surname>Volk</surname>
<given-names>C</given-names></string-name>, et al. <article-title>Sleep EEG slow-wave activity in medicated and unmedicated children and adolescents with attention-deficit/hyperactivity disorder</article-title>. <source><italic toggle="yes">Transl Psychiatry</italic></source>. <year>2019</year>;<volume>9</volume>(<issue>1</issue>):<fpage>324</fpage>. doi:<pub-id pub-id-type="doi">10.1038/s41398-019-0659-3</pub-id><pub-id pub-id-type="pmid">31780639</pub-id></mixed-citation>
      </ref>
      <ref id="cit0006">
        <label>6.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Diep</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Garcia-Molina</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Jasko</surname>
<given-names>J</given-names></string-name>, et al. <article-title>Acoustic enhancement of slow wave sleep on consecutive nights improves alertness and attention in chronically short sleepers</article-title>. <source><italic toggle="yes">Sleep Med</italic></source>. <year>2021</year>;<volume>81</volume>:<fpage>69</fpage>–<lpage>79</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.sleep.2021.01.044</pub-id><pub-id pub-id-type="pmid">33639484</pub-id></mixed-citation>
      </ref>
      <ref id="cit0007">
        <label>7.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Ferrarelli</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Kaskie</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Laxminarayan</surname>
<given-names>S</given-names></string-name>, et al. <article-title>An increase in sleep slow waves predicts better working memory performance in healthy individual</article-title>. <source><italic toggle="yes">Neuroimage</italic></source>. <year>2019</year>;<volume>191</volume>:<fpage>1</fpage>–<lpage>9</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.neuroimage.2019.02.020</pub-id><pub-id pub-id-type="pmid">30753924</pub-id></mixed-citation>
      </ref>
      <ref id="cit0008">
        <label>8.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Fernandez</surname>
<given-names>LMJ</given-names></string-name>, <string-name><surname>Lüthi</surname>
<given-names>A</given-names></string-name>. <article-title>Sleep spindles: mechanisms and functions</article-title>. <source><italic toggle="yes">Physiol Rev</italic></source>. <year>2020</year>;<volume>100</volume>(<issue>2</issue>):<fpage>805</fpage>–<lpage>868</lpage>. doi:<pub-id pub-id-type="doi">10.1152/physrev.00042.2018</pub-id><pub-id pub-id-type="pmid">31804897</pub-id></mixed-citation>
      </ref>
      <ref id="cit0009">
        <label>9.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Pushpanathan</surname>
<given-names>ME</given-names></string-name>, <string-name><surname>Loftus</surname>
<given-names>AM</given-names></string-name>, <string-name><surname>Thomas</surname>
<given-names>MG</given-names></string-name>, et al. <article-title>The relationship between sleep and cognition in Parkinson’s disease: a meta-analysis</article-title>. <source><italic toggle="yes">Sleep Med Rev</italic></source>. <year>2016</year>;<volume>26</volume>:<fpage>21</fpage>–<lpage>32</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.smrv.2015.04.003</pub-id><pub-id pub-id-type="pmid">26365136</pub-id></mixed-citation>
      </ref>
      <ref id="cit0010">
        <label>10.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Zhang</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Zhong</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Li</surname>
<given-names>S</given-names></string-name>, et al. <article-title>Alteration in sleep architecture and electroencephalogram as an early sign of Alzheimer’s disease preceding the disease pathology and cognitive decline</article-title>. <source><italic toggle="yes">Alzheimers Dement</italic></source>. <year>2019</year>;<volume>15</volume>(<issue>4</issue>):<fpage>590</fpage>–<lpage>597</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jalz.2018.12.004</pub-id><pub-id pub-id-type="pmid">30819626</pub-id></mixed-citation>
      </ref>
      <ref id="cit0011">
        <label>11.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Bartsch</surname>
<given-names>U</given-names></string-name>, <string-name><surname>Simpkin</surname>
<given-names>AJ</given-names></string-name>, <string-name><surname>Demanuele</surname>
<given-names>C</given-names></string-name>, et al. <article-title>Distributed slow-wave dynamics during sleep predict memory consolidation and its impairment in schizophrenia</article-title>. <source><italic toggle="yes">NPJ Schizophr</italic></source>. <year>2019</year>;<volume>5</volume>(<issue>1</issue>):<fpage>18</fpage>. doi:<pub-id pub-id-type="doi">10.1038/s41537-019-0086-8</pub-id><pub-id pub-id-type="pmid">31685816</pub-id></mixed-citation>
      </ref>
      <ref id="cit0012">
        <label>12.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Markovic</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Buckley</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Driver</surname>
<given-names>DI</given-names></string-name>, et al. <article-title>Sleep spindle activity in childhood onset schizophrenia: diminished and associated with clinical symptoms</article-title>. <source><italic toggle="yes">Schizophr Res</italic></source>. <year>2020</year>;<volume>223</volume>:<fpage>327</fpage>–<lpage>336</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.schres.2020.08.022</pub-id><pub-id pub-id-type="pmid">32980206</pub-id></mixed-citation>
      </ref>
      <ref id="cit0013">
        <label>13.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Boostani</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Karimzadeh</surname>
<given-names>F</given-names></string-name>, <string-name><surname>Nami</surname>
<given-names>M</given-names></string-name>. <article-title>A comparative review on sleep stage classification methods in patients and healthy individuals</article-title>. <source><italic toggle="yes">Comput Methods Programs Biomed</italic></source>. <year>2017</year>;<volume>140</volume>:<fpage>77</fpage>–<lpage>91</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.cmpb.2016.12.004</pub-id><pub-id pub-id-type="pmid">28254093</pub-id></mixed-citation>
      </ref>
      <ref id="cit0014">
        <label>14.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Fonseca</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Long</surname>
<given-names>X</given-names></string-name>, <string-name><surname>Radha</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Haakma</surname>
<given-names>R</given-names></string-name>, <string-name><surname>Aarts</surname>
<given-names>RM</given-names></string-name>, <string-name><surname>Rolink</surname>
<given-names>J</given-names></string-name>. <article-title>Sleep stage classification with ECG and respiratory effort</article-title>. <source><italic toggle="yes">Physiol Meas</italic></source>. <year>2015</year>;<volume>36</volume>(<issue>10</issue>):<fpage>2027</fpage>–<lpage>2040</lpage>. doi:<pub-id pub-id-type="doi">10.1088/0967-3334/36/10/2027</pub-id><pub-id pub-id-type="pmid">26289580</pub-id></mixed-citation>
      </ref>
      <ref id="cit0015">
        <label>15.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Shi</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Liu</surname>
<given-names>X</given-names></string-name>, <string-name><surname>Li</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>Q</given-names></string-name>, <string-name><surname>Li</surname>
<given-names>Y</given-names></string-name>, <string-name><surname>Ying</surname>
<given-names>S</given-names></string-name>. <article-title>Multi-channel EEG-based sleep stage classification with joint collaborative representation and multiple kernel learning</article-title>. <source><italic toggle="yes">J Neurosci Methods</italic></source>. <year>2015</year>;<volume>254</volume>:<fpage>94</fpage>–<lpage>101</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.jneumeth.2015.07.006</pub-id><pub-id pub-id-type="pmid">26192325</pub-id></mixed-citation>
      </ref>
      <ref id="cit0016">
        <label>16.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Sousa</surname>
<given-names>T</given-names></string-name>, <string-name><surname>Cruz</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Khalighi</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Pires</surname>
<given-names>G</given-names></string-name>, <string-name><surname>Nunes</surname>
<given-names>U</given-names></string-name>. <article-title>A two-step automatic sleep stage classification method with dubious range detection</article-title>. <source><italic toggle="yes">Comput Biol Med</italic></source>. <year>2015</year>;<volume>59</volume>:<fpage>42</fpage>–<lpage>53</lpage>. doi:<pub-id pub-id-type="doi">10.1016/j.compbiomed.2015.01.017</pub-id><pub-id pub-id-type="pmid">25677576</pub-id></mixed-citation>
      </ref>
      <ref id="cit0017">
        <label>17.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Biswal</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Kulas</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Sun</surname>
<given-names>H</given-names></string-name>, et al. <article-title>SLEEPNET: automated sleep staging system via deep learning</article-title>. <source><italic toggle="yes">arXvi</italic></source>. <year>2017</year>. doi:<pub-id pub-id-type="doi">10.48550/arxiv.1707.08262</pub-id></mixed-citation>
      </ref>
      <ref id="cit0018">
        <label>18.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Sun</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Chen</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Li</surname>
<given-names>W</given-names></string-name>, <string-name><surname>Fan</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Chen</surname>
<given-names>W</given-names></string-name>. <article-title>A hierarchical neural network for sleep stage classification based on comprehensive feature learning and multi-flow sequence learning</article-title>. <source><italic toggle="yes">IEEE J Biomed Health Inform</italic></source>. <year>2020</year>;<volume>24</volume>(<issue>5</issue>):<fpage>1351</fpage>–<lpage>1366</lpage>. doi:<pub-id pub-id-type="doi">10.1109/JBHI.2019.2937558</pub-id><pub-id pub-id-type="pmid">31478877</pub-id></mixed-citation>
      </ref>
      <ref id="cit0019">
        <label>19.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Xu</surname>
<given-names>ZL</given-names></string-name>, <string-name><surname>Yang</surname>
<given-names>XJ</given-names></string-name>, <string-name><surname>Sun</surname>
<given-names>JB</given-names></string-name>, <string-name><surname>Liu</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Qin</surname>
<given-names>W</given-names></string-name>. <article-title>Sleep stage classification using time-frequency spectra from consecutive multi-time points</article-title>. <source><italic toggle="yes">Front Neurosci</italic></source>. <year>2020</year>;<volume>14</volume>:<fpage>14</fpage>. doi:<pub-id pub-id-type="doi">10.3389/fnins.2020.00014</pub-id><pub-id pub-id-type="pmid">32047422</pub-id></mixed-citation>
      </ref>
      <ref id="cit0020">
        <label>20.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Patanaik</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Ong</surname>
<given-names>JL</given-names></string-name>, <string-name><surname>Gooley</surname>
<given-names>JJ</given-names></string-name>, <string-name><surname>Ancoli-Israel</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Chee</surname>
<given-names>MWL</given-names></string-name>. <article-title>An end-to-end framework for real-time automatic sleep stage classification</article-title>. <source><italic toggle="yes">Sleep</italic></source>. <year>2018</year>;<volume>41</volume>(<issue>5</issue>):<fpage>1</fpage>–<lpage>11</lpage>. doi:<pub-id pub-id-type="doi">10.1093/sleep/zsy041</pub-id></mixed-citation>
      </ref>
      <ref id="cit0021">
        <label>21.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Stephansen</surname>
<given-names>JB</given-names></string-name>, <string-name><surname>Olesen</surname>
<given-names>AN</given-names></string-name>, <string-name><surname>Olsen</surname>
<given-names>M</given-names></string-name>, et al. <article-title>Neural network analysis of sleep stages enables efficient diagnosis of narcolepsy</article-title>. <source><italic toggle="yes">Nat Commun</italic></source>. <year>2018</year>;<volume>9</volume>(<issue>1</issue>):<fpage>5229</fpage>. doi:<pub-id pub-id-type="doi">10.1038/s41467-018-07229-3</pub-id><pub-id pub-id-type="pmid">30523329</pub-id></mixed-citation>
      </ref>
      <ref id="cit0022">
        <label>22.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Biswal</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Sun</surname>
<given-names>H</given-names></string-name>, <string-name><surname>Goparaju</surname>
<given-names>B</given-names></string-name>, <string-name><surname>Westover</surname>
<given-names>MB</given-names></string-name>, <string-name><surname>Sun</surname>
<given-names>J</given-names></string-name>, <string-name><surname>Bianchi</surname>
<given-names>MT</given-names></string-name>. <article-title>Expert-level sleep scoring with deep neural networks</article-title>. <source><italic toggle="yes">J Am Med Informatics Assoc</italic></source>. <year>2018</year>;<volume>25</volume>(<issue>12</issue>):<fpage>1643</fpage>–<lpage>1650</lpage>. doi:<pub-id pub-id-type="doi">10.1093/jamia/ocy131</pub-id></mixed-citation>
      </ref>
      <ref id="cit0023">
        <label>23.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Perslev</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Darkner</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Kempfner</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Nikolic</surname>
<given-names>M</given-names></string-name>, <string-name><surname>Jennum</surname>
<given-names>PJ</given-names></string-name>, <string-name><surname>Igel</surname>
<given-names>C</given-names></string-name>. <article-title>U-sleep: resilient high-frequency sleep staging</article-title>. <source><italic toggle="yes">NPJ Digit Med</italic></source>. <year>2021</year>;<volume>4</volume>(<issue>1</issue>):<fpage>72</fpage>. doi:<pub-id pub-id-type="doi">10.1038/s41746-021-00440-5</pub-id><pub-id pub-id-type="pmid">33859353</pub-id></mixed-citation>
      </ref>
      <ref id="cit0024">
        <label>24.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Olesen</surname>
<given-names>AN</given-names></string-name>, <string-name><surname>Jørgen Jennum</surname>
<given-names>P</given-names></string-name>, <string-name><surname>Mignot</surname>
<given-names>E</given-names></string-name>, <string-name><surname>Sorensen</surname>
<given-names>HBD</given-names></string-name>. <article-title>Automatic sleep stage classification with deep residual networks in a mixed-cohort setting</article-title>. <source><italic toggle="yes">Sleep</italic></source>. <year>2021</year>;<volume>44</volume>(<issue>1</issue>):<fpage>zsaa161</fpage>. doi:<pub-id pub-id-type="doi">10.1093/sleep/zsaa161</pub-id><pub-id pub-id-type="pmid">32844179</pub-id></mixed-citation>
      </ref>
      <ref id="cit0025">
        <label>25.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Hochreiter</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Schmidhuber</surname>
<given-names>J</given-names></string-name>. <article-title>Long short-term memory</article-title>. <source><italic toggle="yes">Neural Comput</italic></source>. <year>1997</year>;<volume>9</volume>(<issue>8</issue>):<fpage>1735</fpage>–<lpage>1780</lpage>. doi:<pub-id pub-id-type="doi">10.1162/neco.1997.9.8.1735</pub-id><pub-id pub-id-type="pmid">9377276</pub-id></mixed-citation>
      </ref>
      <ref id="cit0026">
        <label>26.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Zhang</surname>
<given-names>GQ</given-names></string-name>, <string-name><surname>Cui</surname>
<given-names>L</given-names></string-name>, <string-name><surname>Mueller</surname>
<given-names>R</given-names></string-name>, et al. <article-title>The National Sleep Research Resource: towards a sleep data commons</article-title>. <source><italic toggle="yes">J Am Med Inform Assoc</italic></source>. <year>2018</year>;<volume>25</volume>(<issue>10</issue>):<fpage>1351</fpage>–<lpage>1358</lpage>. doi:<pub-id pub-id-type="doi">10.1093/jamia/ocy064</pub-id><pub-id pub-id-type="pmid">29860441</pub-id></mixed-citation>
      </ref>
      <ref id="cit0027">
        <label>27.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Quan</surname>
<given-names>SF</given-names></string-name>, <string-name><surname>Howard</surname>
<given-names>BV</given-names></string-name>, <string-name><surname>Iber</surname>
<given-names>C</given-names></string-name>, et al. <article-title>The sleep heart health study: design, rationale, and methods</article-title>. <source><italic toggle="yes">Sleep</italic></source>. <year>1997</year>;<volume>20</volume>(<issue>12</issue>):<fpage>1077</fpage>–<lpage>1085</lpage>.<pub-id pub-id-type="pmid">9493915</pub-id></mixed-citation>
      </ref>
      <ref id="cit0028">
        <label>28.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Rosen</surname>
<given-names>CL</given-names></string-name>, <string-name><surname>Larkin</surname>
<given-names>EK</given-names></string-name>, <string-name><surname>Kirchner</surname>
<given-names>HL</given-names></string-name>, et al. <article-title>Prevalence and risk factors for sleep-disordered breathing in 8- to 11-year-old children: association with race and prematurity</article-title>. <source><italic toggle="yes">J Pediatr</italic></source>. <year>2003</year>;<volume>142</volume>(<issue>4</issue>):<fpage>383</fpage>–<lpage>389</lpage>. doi:<pub-id pub-id-type="doi">10.1067/mpd.2003.28</pub-id><pub-id pub-id-type="pmid">12712055</pub-id></mixed-citation>
      </ref>
      <ref id="cit0029">
        <label>29.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Krizhevsky</surname>
<given-names>A</given-names></string-name>, <string-name><surname>Sutskever</surname>
<given-names>I</given-names></string-name>, <string-name><surname>Hinton</surname>
<given-names>GE</given-names></string-name>. <article-title>ImageNet classification with deep convolutional neural networks</article-title>. <source><italic toggle="yes">Commun ACM</italic></source>. <year>2017</year>;<volume>60</volume>(<issue>6</issue>):<fpage>84</fpage>–<lpage>90</lpage>. doi:<pub-id pub-id-type="doi">10.1145/3065386</pub-id></mixed-citation>
      </ref>
      <ref id="cit0030">
        <label>30.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Simonyan</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Zisserman</surname>
<given-names>A</given-names></string-name>. <article-title>Very deep convolutional networks for large-scale image recognition</article-title>. <source><italic toggle="yes">arXiv</italic></source>. <year>2015</year>. doi:<pub-id pub-id-type="doi">10.48550/arXiv.1409.1556</pub-id></mixed-citation>
      </ref>
      <ref id="cit0031">
        <label>31.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Szegedy</surname>
<given-names>C</given-names></string-name>, <string-name><surname>Liu</surname>
<given-names>W</given-names></string-name>, <string-name><surname>Jia</surname>
<given-names>YQ</given-names></string-name>, et al. <article-title>Going deeper with convolutions</article-title>. <source><italic toggle="yes">arXiv</italic></source>. <year>2014</year>. doi:<pub-id pub-id-type="doi">10.48550/arXiv.1409.4842</pub-id></mixed-citation>
      </ref>
      <ref id="cit0032">
        <label>32.</label>
        <mixed-citation publication-type="journal"><string-name><surname>He</surname>
<given-names>K</given-names></string-name>, <string-name><surname>Zhang</surname>
<given-names>XY</given-names></string-name>, <string-name><surname>Ren</surname>
<given-names>SQ</given-names></string-name>, <string-name><surname>Sun</surname>
<given-names>J</given-names></string-name>. <article-title>Deep residual learning for image recognition</article-title>. <source><italic toggle="yes">arXiv</italic></source>. <year>2015</year>. doi:<pub-id pub-id-type="doi">10.48550/arXiv.1512.03385</pub-id></mixed-citation>
      </ref>
      <ref id="cit0033">
        <label>33.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Xie</surname>
<given-names>S</given-names></string-name>, <string-name><surname>Tu</surname>
<given-names>Z</given-names></string-name>. <article-title>Holistically-nested edge detection</article-title>. <source><italic toggle="yes">Int J Comput Vis</italic></source>. <year>2017</year>;<volume>125</volume>(<issue>1–3</issue>):<fpage>3</fpage>–<lpage>18</lpage>. doi:<pub-id pub-id-type="doi">10.1007/s11263-017-1004-z</pub-id></mixed-citation>
      </ref>
      <ref id="cit0034">
        <label>34.</label>
        <mixed-citation publication-type="journal"><string-name><surname>Tetko</surname>
<given-names>IV</given-names></string-name>, <string-name><surname>Livingstone</surname>
<given-names>DJ</given-names></string-name>, <string-name><surname>Luik</surname>
<given-names>AI</given-names></string-name>. <article-title>Neural network studies. 1. comparison of overfitting and overtraining</article-title>. <source><italic toggle="yes">J Chem Inf Comput Sci</italic></source>. <year>1995</year>;<volume>35</volume>(<issue>5</issue>):<fpage>826</fpage>–<lpage>833</lpage>. doi:<pub-id pub-id-type="doi">10.1021/ci00027a006</pub-id></mixed-citation>
      </ref>
    </ref-list>
  </back>
</article>
</pmc-articleset>
